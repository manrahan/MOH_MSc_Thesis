{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f213685d-a844-4dae-8f85-56b4c860cbe0",
   "metadata": {
    "id": "f213685d-a844-4dae-8f85-56b4c860cbe0",
    "tags": []
   },
   "source": [
    "# Training a Random Forest Model over a CORINE Dataset using Google Earth Engine with the Python API\n",
    "\n",
    "This Jupyter Notebook demonstrates how to train a random forest model over a CORINE dataset using Google Earth Engine with the Python API. The goal is to classify land cover categories based on the CORINE data and create decision trees using the random forest algorithm. We will also use geemap to visualize the results and save the decision trees locally.\n",
    "\n",
    "## Workflow Overview\n",
    "The workflow consists of the following steps:\n",
    "\n",
    "1. Set up the environment: Import the necessary libraries and authenticate our Earth Engine account.\n",
    "\n",
    "2. Define the study area: Use GEE assets (tables or shapefiles) to define the study area.\n",
    "\n",
    "3. Loop through the years: Train over the relevant Corine data by looping through the years.\n",
    "\n",
    "4. Return the created decision trees: Return the created decision trees with accuracy assessment details.\n",
    "\n",
    "5. Saving the classifier will allow us to classify a more continuous timeseries.\n",
    "\n",
    "## Useful to Know: \n",
    "\n",
    "- Here, the classifiers are saved locally for re-use. Unfortunately, due to server limits, a trained model cannot exceed 10MB before memory limits are encountered.\n",
    "\n",
    "- The underlying landsat timeseries is created using an adaptation of the landTrendr module, adapted and simplified here for efficiency. It accomplishes creating the image series and band calculation. This version uses the latest processing efforts of the Landsat TM+ ETM+ and OLI collection 2.\n",
    "\n",
    "- The classification routine has been implemented here by Mike O'Hanrahan for their MSc graduation thesis project.\n",
    "\n",
    "This workflow provides a useful tool for classifying land cover categories based on the CORINE dataset, making it useful for various applications such as land use and land cover change analysis, environmental monitoring, and more. The workflow is flexible and can be adapted for different datasets and study areas. By using geemap for visualizing the results and saving the decision trees locally, this notebook provides an easy-to-use and efficient method for training random forest models over large geospatial datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c4bce2-93f0-496c-b8b6-c705773e99d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is:  2023-04-24 12:09:58.811103\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import geemap.ml as ml\n",
    "from ipygee import chart as chart\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "today = dt.today()\n",
    "print(\"Today is: \", today)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d87b6-10ca-4e50-b416-37c1ec6d6b58",
   "metadata": {},
   "source": [
    "## GEE Authentication\n",
    "\n",
    "Before using the Earth Engine Python API, we need to authenticate our account. The authentication step is required for the first time you use Earth Engine in a new session and roughly every week thereafter.\n",
    "\n",
    "To authenticate, run the following cell and follow the prompts to log into your Earth Engine account. You will then be prompted to copy and paste the authentication code into the box provided. Once you have pasted the code, press enter to save the token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4ac507-28df-4313-a67c-2db9fb8681ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ee.Authenticate()\n",
    "geemap.ee_initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595b921a-da92-4583-961c-4d5cf3dad654",
   "metadata": {},
   "source": [
    "## Hydroclimatic Information\n",
    "\n",
    "The code reads a CSV file containing a list of catchment IDs and stores them as a list. The number of catchments in the list is printed to the console, followed by the list of catchment IDs. The code is designed to process hydroclimatic variables for a given set of catchments. The path and version of the input file are defined as variables at the beginning of the code. The CSV file contains a column called \"ID\" that lists the catchment IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dce165c-0e65-44be-b541-994dd9b4a4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 catchments processed for hydroclimatic variables:\n",
      " \n",
      "[17005, 18001, 20007, 21017, 21023, 21024, 22001, 23004, 24004, 25006, 26003, 27035, 27042, 27047, 27051, 27071, 28046, 28072, 29003, 29009, 30004, 30012, 30015, 31023, 32003, 33018, 33019, 33029, 34011, 36003, 36009, 36010, 37005, 38026, 39017, 39019, 39020, 39025, 39034, 40005, 40011, 41022, 41025, 41027, 41029, 42003, 43014, 45005, 46003, 46005, 47009, 48003, 48004, 49004, 50002, 52010, 52016, 53006, 53008, 53009, 53017, 54008, 54018, 54025, 54034, 54036, 55008, 55014, 55016, 55026, 55029, 56013, 57004, 60002, 60003, 62001, 64001, 65005, 67010, 67018, 68005, 71001, 71004, 72005, 73005, 73011, 75017, 76014, 77004, 78004, 79002, 79004, 8009, 93001, 94001]\n"
     ]
    }
   ],
   "source": [
    "# set the path and version of the input data\n",
    "p = '..'\n",
    "version = 'Version_3_20230303'\n",
    "\n",
    "# read in the list of catchment IDs from the input csv file\n",
    "l = pd.read_csv(f\"{p}/Inputs/{version}/GB.csv\").ID\n",
    "\n",
    "# display the list of catchment IDs and convert it to a Python list\n",
    "names = l.tolist()\n",
    "\n",
    "# print the number of catchments and their IDs\n",
    "print(f'{len(names)} catchments processed for hydroclimatic variables:\\n \\n{names}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d6a8b5-c6ae-4717-9217-99abc0df58bf",
   "metadata": {},
   "source": [
    "## Load the JS module\n",
    "\n",
    "This notebook uses an adapted landTrendr package to construct time series of Landsat imagery for land cover detection. The package is optimized for deforestation event detection and can be used with the latest version of landTrendr available in the GEE asset, which has an Apache license and is free to use.\n",
    "\n",
    "To load the package, we use the ltgee.buildSRcollection method. Note that if the JavaScript module is faulty, the cell below will not load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c702bf6-33c6-4c28-ba17-2eb15870d719",
   "metadata": {
    "id": "e6157fac-15c5-4016-b24e-3729960e6844"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT! Please be advised:\n",
      "- This version of the Adapted_LT.js modules\n",
      "  uses some code adapted from the aut/or: @author Justin Braaten (Google) * @author Zhiqiang Yang (USDA Forest Service) * @author Robert Kennedy (Oregon State University)\n",
      "The latest edits to this code occur: 08/03/2023 for the adaptation efforts by @Mike OHanrahan (TU DELFT MSc research)\n"
     ]
    }
   ],
   "source": [
    "oeel = geemap.requireJS()\n",
    "\n",
    "Map = geemap.Map()\n",
    "\n",
    "ltgee = geemap.requireJS(r'../JS_module/Adapted_LT_v7.3.js')\n",
    "\n",
    "#ltgee.availability  #all functions within the javascript module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484acd26-c655-4081-87b5-756c24f92c7a",
   "metadata": {},
   "source": [
    "## Initiate With a Shapefile\n",
    "\n",
    "This notebook assumes the user has a shapefile saved as an asset on their GEE, the assets used in the CATAPUCII project will be made publicly available in the @mohanrahan repository\n",
    "\n",
    "Assigning some useful variables for later classification\n",
    "\n",
    "Catchment Assets are available at this address:\n",
    "\n",
    "https://code.earthengine.google.com/?asset=projects/mohanrahan/assets/CATAPUCII_Catchments\n",
    "\n",
    "\n",
    "## Assigning useful variables\n",
    "\n",
    "- The asset_dir will point to the shapefile loaded as a GEE table asset. \n",
    "- crs is important for reprojection and scaling (which will affect area calculations)\n",
    "- RGB_VIS is for landsat RGB visual parameters\n",
    "- start day defines the beginning of the seasonal composite period\n",
    "- maskThese applies a mask (renders Null/NA/Transparent) to those majority pixels in landsat imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9045fa07-c860-4501-a0a3-77c980967b84",
   "metadata": {
    "id": "103ce65e-6dc2-4309-9cc8-e7374bf57af3"
   },
   "outputs": [],
   "source": [
    "# Directory where assets are stored\n",
    "asset_dir = 'projects/mohanrahan/assets'\n",
    "\n",
    "# Asset ID for catchment boundaries\n",
    "catchment_asset = 'CATAPUCII_Catchments/CAMELS_GB_catchment_boundaries'\n",
    "\n",
    "# Name of the dataset\n",
    "dataset = 'CAMELS_GB'\n",
    "\n",
    "# Column string to identify catchments\n",
    "col_string  = 'ID'\n",
    "\n",
    "# Coordinate reference system, GB is british national grid\n",
    "crs = 'EPSG:27700'\n",
    "\n",
    "# Figure number for plotting\n",
    "fignum = 0\n",
    "\n",
    "# RGB visualization settings for Landsat imagery\n",
    "RGB_VIS = {'bands':['B3','B2','B1'], 'min':0, 'max':1.5e3}\n",
    "\n",
    "#Classified image visualisation\n",
    "lc_vis = {'bands':['landcover'], 'min':1, 'max':5, 'palette':['#E6004D', '#FFFFA8', '#80FF00', '#A6A6FF', '#00CCF2']}\n",
    "\n",
    "#Classified image visualisation\n",
    "class_vis = {'bands':['classification'], 'min':1, 'max':5, 'palette':['#E6004D', '#FFFFA8', '#80FF00', '#A6A6FF', '#00CCF2']}\n",
    "\n",
    "# Start and end years for Landsat data collection\n",
    "startYear = 1984\n",
    "endYear = 2022\n",
    "\n",
    "# Start and end days for Landsat data collection\n",
    "startDay = '06-20'\n",
    "endDay = '08-31'\n",
    "\n",
    "# List of images to be masked from Landsat collection\n",
    "maskThese = ['cloud', 'shadow', 'snow',]\n",
    "\n",
    "# List of bands to include in Landsat collection\n",
    "bandList = [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B7\", \n",
    "           'NBR', 'NDMI', 'NDVI', 'NDSI', 'EVI','GNDVI', \n",
    "           'TCB', 'TCG', 'TCW', 'TCA', 'NDFI',] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24017ee3-b207-447c-938e-200892cfed46",
   "metadata": {},
   "source": [
    "## The Table Data\n",
    "\n",
    "### Code:\n",
    "The code imports a feature collection of catchment boundaries from the Google Earth Engine (GEE) asset directory, calculates the area of each catchment in square kilometers and pixels, sets a unique identifier for each catchment, and filters and sorts the catchment collection based on the area. It then converts the filtered and sorted collection to a pandas dataframe, selects only the catchments that are specified in a list of catchment names, and saves the resulting table to an Excel file.\n",
    "\n",
    "### Summary:\n",
    "The code fetches catchment boundaries from a GEE asset directory and calculates their areas in both square kilometers and pixels. Then, it assigns a unique identifier to each catchment and filters and sorts them based on their area. It saves a subset of the resulting catchment table that contains only the catchments specified in a list of names to an Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "257eab36-74c7-46b1-97e9-3cb1af199dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_area</th>\n",
       "      <th>area_km2</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>VERSION</th>\n",
       "      <th>ID</th>\n",
       "      <th>EXPORTED</th>\n",
       "      <th>ID_STRING</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>system_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00000000000000000088</th>\n",
       "      <td>1355.679450</td>\n",
       "      <td>1349.764903</td>\n",
       "      <td>National River Flow Archive</td>\n",
       "      <td>1.3</td>\n",
       "      <td>27071</td>\n",
       "      <td>1518422400000</td>\n",
       "      <td>27071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000000235</th>\n",
       "      <td>1145.767150</td>\n",
       "      <td>1140.885961</td>\n",
       "      <td>National River Flow Archive</td>\n",
       "      <td>1.3</td>\n",
       "      <td>71001</td>\n",
       "      <td>1518422400000</td>\n",
       "      <td>71001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000000000000000001c0</th>\n",
       "      <td>1125.603290</td>\n",
       "      <td>1121.172318</td>\n",
       "      <td>National River Flow Archive</td>\n",
       "      <td>1.3</td>\n",
       "      <td>54008</td>\n",
       "      <td>1518422400000</td>\n",
       "      <td>54008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000000207</th>\n",
       "      <td>897.948531</td>\n",
       "      <td>894.478359</td>\n",
       "      <td>National River Flow Archive</td>\n",
       "      <td>1.3</td>\n",
       "      <td>62001</td>\n",
       "      <td>1518422400000</td>\n",
       "      <td>62001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000000025a</th>\n",
       "      <td>798.157955</td>\n",
       "      <td>794.507522</td>\n",
       "      <td>National River Flow Archive</td>\n",
       "      <td>1.3</td>\n",
       "      <td>79002</td>\n",
       "      <td>1518422400000</td>\n",
       "      <td>79002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000000021b</th>\n",
       "      <td>12.768404</td>\n",
       "      <td>12.719149</td>\n",
       "      <td>National River Flow Archive</td>\n",
       "      <td>1.3</td>\n",
       "      <td>67010</td>\n",
       "      <td>1518422400000</td>\n",
       "      <td>67010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000000082</th>\n",
       "      <td>11.353532</td>\n",
       "      <td>11.304831</td>\n",
       "      <td>National River Flow Archive</td>\n",
       "      <td>1.3</td>\n",
       "      <td>27047</td>\n",
       "      <td>1518422400000</td>\n",
       "      <td>27047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000000000000000001df</th>\n",
       "      <td>10.506362</td>\n",
       "      <td>10.466687</td>\n",
       "      <td>National River Flow Archive</td>\n",
       "      <td>1.3</td>\n",
       "      <td>55008</td>\n",
       "      <td>1518422400000</td>\n",
       "      <td>55008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000000084</th>\n",
       "      <td>8.177400</td>\n",
       "      <td>8.143164</td>\n",
       "      <td>National River Flow Archive</td>\n",
       "      <td>1.3</td>\n",
       "      <td>27051</td>\n",
       "      <td>1518422400000</td>\n",
       "      <td>27051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000000000000000000c6</th>\n",
       "      <td>4.316773</td>\n",
       "      <td>4.299710</td>\n",
       "      <td>National River Flow Archive</td>\n",
       "      <td>1.3</td>\n",
       "      <td>31023</td>\n",
       "      <td>1518422400000</td>\n",
       "      <td>31023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       pixel_area     area_km2                       SOURCE  \\\n",
       "system_index                                                                  \n",
       "00000000000000000088  1355.679450  1349.764903  National River Flow Archive   \n",
       "00000000000000000235  1145.767150  1140.885961  National River Flow Archive   \n",
       "000000000000000001c0  1125.603290  1121.172318  National River Flow Archive   \n",
       "00000000000000000207   897.948531   894.478359  National River Flow Archive   \n",
       "0000000000000000025a   798.157955   794.507522  National River Flow Archive   \n",
       "...                           ...          ...                          ...   \n",
       "0000000000000000021b    12.768404    12.719149  National River Flow Archive   \n",
       "00000000000000000082    11.353532    11.304831  National River Flow Archive   \n",
       "000000000000000001df    10.506362    10.466687  National River Flow Archive   \n",
       "00000000000000000084     8.177400     8.143164  National River Flow Archive   \n",
       "000000000000000000c6     4.316773     4.299710  National River Flow Archive   \n",
       "\n",
       "                     VERSION     ID       EXPORTED ID_STRING  \n",
       "system_index                                                  \n",
       "00000000000000000088     1.3  27071  1518422400000     27071  \n",
       "00000000000000000235     1.3  71001  1518422400000     71001  \n",
       "000000000000000001c0     1.3  54008  1518422400000     54008  \n",
       "00000000000000000207     1.3  62001  1518422400000     62001  \n",
       "0000000000000000025a     1.3  79002  1518422400000     79002  \n",
       "...                      ...    ...            ...       ...  \n",
       "0000000000000000021b     1.3  67010  1518422400000     67010  \n",
       "00000000000000000082     1.3  27047  1518422400000     27047  \n",
       "000000000000000001df     1.3  55008  1518422400000     55008  \n",
       "00000000000000000084     1.3  27051  1518422400000     27051  \n",
       "000000000000000000c6     1.3  31023  1518422400000     31023  \n",
       "\n",
       "[95 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "# Define the feature collection from asset directory and catchment asset name\n",
    "table = ee.FeatureCollection(f\"{asset_dir}/{catchment_asset}\")\n",
    "\n",
    "# Define a function to calculate the area of each geometry in square kilometer\n",
    "def set_area_km2(feature):\n",
    "    '''\n",
    "    Calculate the area of each geometry in square kilometer\n",
    "    '''\n",
    "    area = feature.geometry().area().divide(1000*1000)\n",
    "    setting = feature.set('area_km2', area)\n",
    "    return setting\n",
    "\n",
    "# Define a function to calculate the area of each geometry in pixels\n",
    "def set_area_pixel(feature):\n",
    "    '''\n",
    "    Calculate the area of each geometry in pixels\n",
    "    '''\n",
    "    aoi = feature.geometry()\n",
    "    area = ee.Image.pixelArea().divide(1e6).clip(aoi).select('area').reduceRegion(**{\n",
    "        'reducer':ee.Reducer.sum(),\n",
    "        'geometry':aoi,\n",
    "        'scale':30,\n",
    "        'crs':crs,\n",
    "        'maxPixels':1e13,\n",
    "        'bestEffort':True,\n",
    "        }).get('area')\n",
    "    setting = feature.set('pixel_area', area)\n",
    "    return setting\n",
    "\n",
    "# Define a function to set the system ID as a column\n",
    "def set_id(feature):\n",
    "    '''\n",
    "    Set the system ID as a column\n",
    "    '''\n",
    "    getting_name = ee.String(feature.get('system:index'))\n",
    "    setting_id = feature.set({'system_index':getting_name,})\n",
    "    return setting_id\n",
    "\n",
    "# Calculate the area of each geometry and set ID column and pixel area column\n",
    "table_area = table.map(set_area_km2).map(set_id).map(set_area_pixel)\n",
    "\n",
    "# Filter out geometries with area_km2 equal to zero and sort by area from largest to smallest\n",
    "Filtered_Sorted = table_area.filter(ee.Filter.gt('area_km2', 0)).sort('area_km2', False)\n",
    "\n",
    "# Convert the sorted table to a Pandas dataframe and set the index to 'system_index'\n",
    "down = geemap.ee_to_pandas(Filtered_Sorted).set_index(['system_index'])\n",
    "\n",
    "# Select the rows of 'down' where the column specified by 'col_string' is in the list 'names'\n",
    "df1 = down.loc[down[col_string].isin(names)]\n",
    "\n",
    "# Get the system_index values as a list\n",
    "sys_index = df1.index.to_list()\n",
    "\n",
    "# Display the filtered table\n",
    "display(df1)\n",
    "\n",
    "# Print the number of features in the filtered table\n",
    "print(len(df1))\n",
    "\n",
    "# Convert the filtered and sorted table to a Pandas dataframe\n",
    "gdf = geemap.ee_to_pandas(Filtered_Sorted)\n",
    "\n",
    "# Create a directory for the output if it doesn't exist\n",
    "if not os.path.exists(f'../Outputs/{dataset}/'):\n",
    "    os.makedirs(f'../Outputs/{dataset}/')\n",
    "\n",
    "# Export the filtered and sorted table to an Excel file\n",
    "gdf.to_excel(f'../Outputs/{dataset}/{dataset}_catchment_table.xlsx', index=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254a7385-a388-4e13-b0d5-345f74833764",
   "metadata": {},
   "source": [
    "# Defining the zones in the GB Region for classification\n",
    "\n",
    "- if the catchment in question intersects the north mid or south polygon, then the clssified asset is created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cec8f4c-3729-462e-94e8-78020f018376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i north_zone\n",
      "i middle_zone\n",
      "i south_zone\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960f56cd17444ea88af425cf0e6b45a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[53.49722684216607, -2.3029239738111555], controls=(ZoomControl(options=['position', 'zoom_in_text'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "geom_north = {\n",
    "  \"geodesic\": False,\n",
    "  \"type\": \"Polygon\",\n",
    "  \"coordinates\": [\n",
    "    [\n",
    "      [\n",
    "        -5.735496279653574,\n",
    "        56.12064673783441\n",
    "      ],\n",
    "      [\n",
    "        -3.5711896390285736,\n",
    "        56.12064673783441\n",
    "      ],\n",
    "      [\n",
    "        -3.5711896390285736,\n",
    "        57.83265981722564\n",
    "      ],\n",
    "      [\n",
    "        -5.735496279653574,\n",
    "        57.83265981722564\n",
    "      ],\n",
    "      [\n",
    "        -5.735496279653574,\n",
    "        56.12064673783441\n",
    "      ]\n",
    "    ]\n",
    "  ]\n",
    "}\n",
    "\n",
    "geom_middle = {\n",
    "  \"geodesic\": False,\n",
    "  \"type\": \"Polygon\",\n",
    "  \"coordinates\": [\n",
    "    [\n",
    "      [\n",
    "        -4.439109560903574,\n",
    "        53.67024054287634\n",
    "      ],\n",
    "      [\n",
    "        -0.26430487340357356,\n",
    "        53.67024054287634\n",
    "      ],\n",
    "      [\n",
    "        -0.26430487340357356,\n",
    "        56.08388347961458\n",
    "      ],\n",
    "      [\n",
    "        -4.439109560903574,\n",
    "        56.08388347961458\n",
    "      ],\n",
    "      [\n",
    "        -4.439109560903574,\n",
    "        53.67024054287634\n",
    "      ]\n",
    "    ]\n",
    "  ]\n",
    "}\n",
    "\n",
    "geom_south = {\n",
    "  \"geodesic\": False,\n",
    "  \"type\": \"Polygon\",\n",
    "  \"coordinates\": [\n",
    "    [\n",
    "      [\n",
    "        -5.372947451528574,\n",
    "        50.23970424688909\n",
    "      ],\n",
    "      [\n",
    "        1.1694109469089264,\n",
    "        50.23970424688909\n",
    "      ],\n",
    "      [\n",
    "        1.1694109469089264,\n",
    "        53.458178051023836\n",
    "      ],\n",
    "      [\n",
    "        -5.372947451528574,\n",
    "        53.458178051023836\n",
    "      ],\n",
    "      [\n",
    "        -5.372947451528574,\n",
    "        50.23970424688909\n",
    "      ]\n",
    "    ]\n",
    "  ]\n",
    "}\n",
    "\n",
    "geom_tup_ls = [('north_zone', geom_north), ('middle_zone', geom_middle), ('south_zone',geom_south)]\n",
    "\n",
    "aoi = ee.Geometry(geom_middle)\n",
    "\n",
    "catchments = Filtered_Sorted.filter(ee.Filter.inList(col_string, ee.List(names)))\n",
    "\n",
    "def aoi_list_to_df(catchments, tup):\n",
    "    '''\n",
    "    Takes a list of tuples containing [(zone names, geojson geometry),...]\n",
    "    '''\n",
    "    for i, j in tup:\n",
    "        print('i', i)\n",
    "        \n",
    "        aoi = ee.Geometry(j)\n",
    "        \n",
    "        # Convert the filtered and sorted table to a Pandas dataframe\n",
    "        gdf = geemap.ee_to_pandas(catchments.filter(ee.Filter.bounds(aoi)))\n",
    "        \n",
    "        # Export the filtered and sorted table to an Excel file\n",
    "        gdf.to_excel(f'../Outputs/{dataset}/{dataset}_{i}_catchment_table.xlsx', index=0)\n",
    "\n",
    "aoi_list_to_df(catchments, geom_tup_ls)\n",
    "\n",
    "Map = geemap.Map()\n",
    "\n",
    "Map.setOptions('TERRAIN')\n",
    "Map.addLayer(catchments, {'color': 'green'}, 'green: Included')\n",
    "Map.addLayer(catchments.filter(ee.Filter.bounds(aoi).Not()), {'color':'red'}, 'red: Not Included')\n",
    "Map.centerObject(Filtered_Sorted, 7)\n",
    "Map.addLayer(aoi, {'color':'yellow'}, 'area trained')\n",
    "\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fec43c4c-115d-4483-bb47-8c8994832a42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clip_collection(image: ee.Image) -> ee.Image:\n",
    "    '''\n",
    "    Clip an image to the area of interest and return a copy of the clipped image.\n",
    "\n",
    "    Input:\n",
    "        image (ee.Image): the image to clip\n",
    "    \n",
    "    Output:\n",
    "        ee.Image: the clipped image\n",
    "    '''\n",
    "    return image.clip(aoi).copyProperties(image)\n",
    "\n",
    "\n",
    "def extractArea(item) -> ee.List:\n",
    "    '''\n",
    "    Extract the area of a classified land cover class as a list of class number and area.\n",
    "\n",
    "    Input:\n",
    "        item (ee.Dictionary): a dictionary containing classification and pixel area information\n",
    "    \n",
    "    Output:\n",
    "        ee.List: a list of class number and area\n",
    "    '''\n",
    "    \n",
    "    areaDict = ee.Dictionary(item)\n",
    "    classNumber = ee.Number(areaDict.get('classification')).format()\n",
    "    area = ee.Number(areaDict.get('sum')).divide(1e6)\n",
    "    return ee.List([classNumber, area])\n",
    "\n",
    "\n",
    "def classArea(classified_image: ee.Image, scale: int) -> ee.List:\n",
    "    '''\n",
    "    Calculate the area of each classified land cover class for an image at a given scale.\n",
    "\n",
    "    Input:\n",
    "        classified_image (ee.Image): classified image\n",
    "        scale (int): scale of the image in meters\n",
    "    \n",
    "    Output:\n",
    "        ee.List: a list of class numbers and areas for the image\n",
    "    '''\n",
    "    \n",
    "    areaImage = ee.Image.pixelArea().addBands(classified_image)\n",
    "    \n",
    "    areas = areaImage.reduceRegion(**{\n",
    "            'reducer':ee.Reducer.sum().group(**{'groupField':1, 'groupName':'classification'}),\n",
    "            'geometry':aoi,\n",
    "            'scale':scale,\n",
    "            'maxPixels':1e10,\n",
    "            'bestEffort':True,\n",
    "    })\n",
    "    \n",
    "    classAreas = ee.List(areas.get('groups'))\n",
    "    \n",
    "    classAreasLists = classAreas.map(extractArea)\n",
    "    \n",
    "    return classAreasLists\n",
    "\n",
    "def msToDate(milliseconds: int) -> datetime.datetime:\n",
    "    '''\n",
    "    Convert a timestamp in milliseconds to a datetime object.\n",
    "\n",
    "    Input:\n",
    "        milliseconds (int): a timestamp in milliseconds\n",
    "    \n",
    "    Output:\n",
    "        datetime.datetime: a datetime object corresponding to the timestamp\n",
    "    '''\n",
    "    \n",
    "    base_datetime = datetime.datetime(1970, 1, 1)\n",
    "    delta = datetime.timedelta(0, 0, 0, milliseconds)\n",
    "    target_datetime = base_datetime + delta\n",
    "    return target_datetime\n",
    "\n",
    "def dataframeAreas(i: int, yc: int, classified: ee.Image, trainingClassImage: ee.Image, ms: int, classImageYear: int, name: str, accuracy: float) -> pd.DataFrame:\n",
    "    '''\n",
    "    Calculate the area of each classified land cover class for an image, given its classification and training images, and return a DataFrame of the results.\n",
    "\n",
    "    Input:\n",
    "        i (int): index of the current image in the collection\n",
    "        yc (int): year count (number of years since the first year in the collection)\n",
    "        classified (ee.Image): classified image\n",
    "        trainingClassImage (ee.Image): training image with land cover classes\n",
    "        ms (int): timestamp of the image in milliseconds\n",
    "        classImageYear (int): year of the image used for classification\n",
    "        name (str): name of the catchment\n",
    "        accuracy (float): accuracy of the classifier used for classification\n",
    "    \n",
    "    Output:\n",
    "        pivoted (pd.DataFrame): a pivoted DataFrame with columns representing land cover classes and rows representing image dates, area of each class for CORINE and random forest classifications, and catchment name, training year, and accuracy information\n",
    "    '''\n",
    "    \n",
    "    ls1 = pd.DataFrame(classArea(classified, 30).getInfo(), columns=['class', 'area_RF'])\n",
    "    ls2 = pd.DataFrame(classArea(trainingClassImage, 100).getInfo(), columns=['class', 'area_CORINE'])\n",
    "\n",
    "    merged = ls1.merge(ls2, how='inner', on='class')\n",
    "    merged['image_date'] = ms\n",
    "    pivoted = merged.pivot(index='image_date', columns='class', values=['area_CORINE', 'area_RF'])\n",
    "    pivoted['training', 'year_trained'] =  classImageYear\n",
    "    pivoted['area_CORINE', '6'] = 0\n",
    "    pivoted['catchment', 'area'] = pivoted.iloc[0, 0:5].sum()\n",
    "    pivoted['area_RF', '6'] = pivoted.catchment.area - pivoted.iloc[0, 6:10].sum() \n",
    "    pivoted['catchment', 'name '] = name\n",
    "    pivoted['testing', 'accuracy'] = accuracy\n",
    "    pivoted['ind'] = str(i)+'_'+str(yc)\n",
    "    pivoted.fillna(0)\n",
    "    \n",
    "    return pivoted\n",
    "\n",
    "\n",
    "    \n",
    "def saveClassifierToCSV(classifier: ee.Classifier, name: str, yc: int) -> None:\n",
    "    '''\n",
    "    Save decision trees from a random forest classifier to CSV files.\n",
    "\n",
    "    Input:\n",
    "        classifier (ee.Classifier): a trained random forest classifier\n",
    "        name (str): name of the catchment\n",
    "        yc (int): year count (number of years since the first year in the collection)\n",
    "    \n",
    "    Output:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    decisionTrees = ee.List(classifier.explain().get('trees')).getInfo()\n",
    "    folder='Trees'\n",
    "    \n",
    "    print('saving...')\n",
    "\n",
    "    var = f'../Outputs/{dataset}/{folder}/'\n",
    "\n",
    "    if not os.path.exists(var):\n",
    "        print('created')\n",
    "        os.makedirs(var)\n",
    "\n",
    "    ml.trees_to_csv(decisionTrees, f'../Outputs/{dataset}/Trees/{name}_{yc}')\n",
    "    \n",
    "def calculate_classification_stats(confusion_matrix):\n",
    "    # Calculate the total number of predictions\n",
    "    total = np.sum(confusion_matrix)\n",
    "    \n",
    "    # Calculate the number of true positives, false positives, false negatives, and true negatives\n",
    "    true_positives = np.diag(confusion_matrix)\n",
    "    false_positives = np.sum(confusion_matrix, axis=0) - true_positives\n",
    "    false_negatives = np.sum(confusion_matrix, axis=1) - true_positives\n",
    "    true_negatives = total - (true_positives + false_positives + false_negatives)\n",
    "\n",
    "    # Calculate the producer and consumer accuracy\n",
    "    producer_accuracy = true_positives / np.sum(confusion_matrix, axis=1)\n",
    "    consumer_accuracy = true_positives / np.sum(confusion_matrix, axis=0)\n",
    "\n",
    "    # Calculate the precision, recall, and F-score\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    fscore = 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "    # Calculate the overall accuracy\n",
    "    overall_accuracy = np.sum(true_positives) / total\n",
    "\n",
    "    # Calculate Cohen's kappa coefficient\n",
    "    observed_agreement = np.sum(true_positives + true_negatives) / total\n",
    "    expected_agreement = np.sum(np.sum(confusion_matrix, axis=0) * np.sum(confusion_matrix, axis=1)) / (total ** 2)\n",
    "    kappa = (observed_agreement - expected_agreement) / (1 - expected_agreement)\n",
    "\n",
    "    # Normalize kappa to a range of -1 to 1\n",
    "    normalized_kappa = kappa / 10\n",
    "\n",
    "    # Return the consumer accuracy, producer accuracy, F-score, overall accuracy, and kappa\n",
    "    return consumer_accuracy, producer_accuracy, fscore, overall_accuracy, normalized_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0209bb49-7f47-40c1-aba7-280cbd0e3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ls = 'used_images'\n",
    "SR_t = 'SR_timeseries'\n",
    "RF_c = 'RF_classification/'\n",
    "Tr = 'Trees'\n",
    "\n",
    "\n",
    "folder_list = [id_ls, SR_t, RF_c, Tr]\n",
    "\n",
    "for folder in folder_list:\n",
    "    \n",
    "    var = f'../Outputs/{dataset}/{folder}'\n",
    "    \n",
    "    if not os.path.exists(var):\n",
    "        print('created')\n",
    "        os.makedirs(var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86deecbc-4c78-450a-b6d0-241bbd355d8f",
   "metadata": {},
   "source": [
    "### areas calculated from: \n",
    "\n",
    "https://code.earthengine.google.com/cc1f2371404daf318b08060def5a1aa5\n",
    "\n",
    "- This code defines a list of land cover class names and three dictionaries of land cover class areas in square kilometers for different regions of interest. \n",
    "- It then extracts the trained area in square kilometers from the area_for_weighted_sample dictionary and calculates the total. \n",
    "- It calculates the percentage of each land cover class within the total trained area and a weight for each land cover class based on its percentage of the total trained area. \n",
    "- Finally, it prints the weights for each land cover class to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4b8e9d7-ec90-4910-97a9-0a2faa063ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points from percentage area:\n",
      "[('Artificial', 169), ('Agricultural', 3166), ('Forest and Semi-Natural', 1286), ('Wetlands', 363), ('Waterbodies', 13)]\n",
      "[0.03390401 0.63330012 0.25724922 0.0727786  0.00276805]\n"
     ]
    }
   ],
   "source": [
    "# Define a list of land cover class names\n",
    "class_names = ['Artificial', 'Agricultural', 'Forest and Semi-Natural', 'Wetlands', 'Waterbodies']\n",
    "\n",
    "full_basin_500 = { #computed at a 500m scale\n",
    "  \"1\": 564.8738188768993,\n",
    "  \"2\": 11118.882881893012,\n",
    "  \"3\": 4459.109223972432,\n",
    "  \"4\": 1292.0259474223649,\n",
    "  \"5\": 48.647703153737744\n",
    "}\n",
    "\n",
    "area_full_100 = { #computed at a 100m scale\n",
    "  \"1\": 591.3541867334484,\n",
    "  \"2\": 11046.02886733322,\n",
    "  \"3\": 4486.944288416541,\n",
    "  \"4\": 1269.405212174017,\n",
    "  \"5\": 48.28035556339614\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "area_for_weighted_sample = area_full_100\n",
    "\n",
    "# Extract the trained area in square kilometers from the area_for_weighted_sample dictionary and calculate the total\n",
    "trained_area_km = [area_for_weighted_sample[i] for i in area_for_weighted_sample.keys()]\n",
    "tot = np.sum(trained_area_km)\n",
    "\n",
    "# Calculate the percentage of each land cover class within the total trained area\n",
    "area_full_pct = np.array([area_for_weighted_sample[i] / tot for i in area_for_weighted_sample.keys()])\n",
    "\n",
    "# Calculate a weight for each land cover class based on its percentage of the total trained area\n",
    "weights = [int(p * 5000) for p in area_full_pct]\n",
    "\n",
    "# Print the weights for each land cover class to the console\n",
    "weight_zip = zip(class_names, weights)\n",
    "print(f'Points from percentage area:\\n{[(cla, point) for cla, point in weight_zip]}')\n",
    "print(area_full_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8b25d-611b-4771-a00c-9db7d876b9b0",
   "metadata": {},
   "source": [
    "## Defining the name of the output\n",
    "- This will dictate whether the distribution of points is weighted and or tuned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bb37397-9edb-4866-acf6-70eaf64f505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_classifier_name = 'GB_Max_untuned'\n",
    "saved_classifier_name1 = 'GB_South_HC_tuned'\n",
    "saved_classifier_name2 = 'GB_Middle_HC_tuned'\n",
    "saved_classifier_name3 = 'GB_North_HC_tuned'\n",
    "\n",
    "scn_list = [saved_classifier_name1, saved_classifier_name2, saved_classifier_name3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3ab3226-6390-4105-adde-ed00fc1ff3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'number_of_trees':120,\n",
    "    'variables_per_split':8,\n",
    "    'minimum_leaf_population':1,\n",
    "    'bag_fraction':0.6,\n",
    "    'max_nodes':330,\n",
    "    'seed':0\n",
    "}\n",
    "\n",
    "\n",
    "classLoopParams = {'dataset':'CORINE',    #training dataset, no other than corine currently supported\n",
    "               'trainingClassLevel':1, #classLevel determines the level of corine class simplification\n",
    "               'customClassLevels':None,   #can provide some custom levels, not fully tested\n",
    "               'numClasses':5,            #if trainingClassLevel is 1 then there are 5 classes, level is 2 then there are 15, 3 is 44. (CORINE land cover class grouping)\n",
    "               'split':0.7,               #split the training and testing 0.7/0.3 (70% training, 30% accuracy testing). \n",
    "               'tileScale':2,            #tileScale higher number reduces likelihood of classifier running into a memory limit\n",
    "                'distribution':'weighted',  # can be weighted or balanced, weighting is done be area proportion\n",
    "               'weighting':list(area_full_pct),  # weighting based on area, percentages are passed , dictionaries do not pass to javascript code as well as lists\n",
    "               'year_classified': [2018],   # classification year is , for classifiers saved, the same as the years available in the training dataset\n",
    "                'hyperparameters': [hyperparameters['number_of_trees'],\n",
    "                                   hyperparameters['variables_per_split'],\n",
    "                                   hyperparameters['minimum_leaf_population'],\n",
    "                                   hyperparameters['bag_fraction'],\n",
    "                                    hyperparameters['max_nodes'],\n",
    "                                   hyperparameters['seed']] #hyperparameters are passed to  via a list, not a dictionary. \n",
    "              }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dddd40-74f7-4634-8072-5d2aed4a85db",
   "metadata": {},
   "source": [
    "## This loop is set to generate all classifiers, it will output CSV classifiers\n",
    "\n",
    "CSV classifiers save locally for all available training years in the corine dataset (or years set by the year classified loop parameter). The training area is the combined geometry of the whole meuse basin and are uploaded from client to server in the form of a decision Tree Ensemble. \n",
    "\n",
    "- Creating the classifier we also need to generate the training accuracy assessing  \"\" confusion matrix \"\"\n",
    "\n",
    "from the confusion matrix: (overall) accuracy, kappa coefficient, producer's accuracy, consumers'accuracy. \n",
    "\n",
    "This will be assessed three times in the report, from a \n",
    "\n",
    "1. balanced non-tuned () [1990, 2000, 2006, 2012, 2018]\n",
    "2. then weighted, non-tuned\n",
    "3. then a weighted tuned classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a90db04-5094-4472-a2f0-d913c471406f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin loop: 2023-04-24 12:10:33.005287\n",
      "2023-04-24 12:10:33.007299\n",
      "Dataset: CAMELS_GB, \n",
      "Training area: GB_North_HC_tuned, \n",
      "Surface Reflectance Processing ...\n",
      "\n",
      "\n",
      "step 2: Initialize classification routine: 2023-04-24 12:10:34.348148\n",
      "classifier is tuned\n",
      "year classified 2018\n",
      "Adding Terrain...\n",
      "Generating GCP...\n",
      "Building classifier...\n",
      "saving the classifier..\n",
      "saving...\n",
      "saved\n",
      "Asessing classifier accuracy...\n",
      "Asessing classifier validation accuracy...\n",
      "Classifying...\n",
      "time for 2018 data = 0:02:09.604496\n",
      "step2: Done: 2023-04-24 12:12:43.952644, time taken: 0:02:09.604496\n",
      "\n",
      "Catchment: GB_North_HC_tuned, total time: 0:02:10.945345\n",
      "---------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ind' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 252\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep2: Done: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt4\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, time taken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt4\u001b[38;5;241m-\u001b[39mt3\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCatchment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, total time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt4\u001b[38;5;241m-\u001b[39mt1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mind\u001b[49m \u001b[38;5;241m==\u001b[39m sys_index[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    255\u001b[0m classArea_df\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Outputs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/RF_classification/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msaved_classifier_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtuned\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_combinedAreas.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ind' is not defined"
     ]
    }
   ],
   "source": [
    "t0 = dt.today()\n",
    "\n",
    "confusion_matrices = []\n",
    "\n",
    "error_matrices = []\n",
    "\n",
    "accuracy_dictionaries = []\n",
    "\n",
    "classArea_df = pd.DataFrame()\n",
    "\n",
    "validation_combined_df = pd.DataFrame()\n",
    "\n",
    "print(f'begin loop: {t0}')\n",
    "\n",
    "\n",
    "for i, (zone_name, geojson) in enumerate(geom_tup_ls[:]):\n",
    "    \n",
    "    aoi = ee.Geometry(geojson)\n",
    "    \n",
    "    area = aoi.area(1).divide(1e6)\n",
    "    \n",
    "    t1 = dt.today()\n",
    "    \n",
    "    name = scn_list[i]\n",
    "    \n",
    "    print(f'{t1}\\nDataset: {dataset}, \\nTraining area: {name}, \\nSurface Reflectance Processing ...\\n')\n",
    "    \n",
    "    annual_med = ltgee.buildSRcollection(startYear, endYear, startDay, endDay, aoi, maskThese, [''])#.map(clip_collection) #much slower when clipped\n",
    "    \n",
    "    annual_med_calc = ltgee.transformSRcollection(annual_med, bandList)\n",
    "    \n",
    "    saved_classifier_name = name\n",
    "\n",
    "    t3 = dt.today()\n",
    "    \n",
    "    print(f'\\nstep 2: Initialize classification routine: {t3}')\n",
    "    \n",
    "    if dataset == 'CAMELS_GB':\n",
    "        '''\n",
    "        \n",
    "        The years that we train on are not necessarily the same as the years we classify: \n",
    "        - first define the years to return that will be relevant to the decadal analysis (matching the hydroclimatic decades)\n",
    "        - Then use conditions to define which training set corresponds best to the image to be classified. \n",
    "        - LATER: Will need to use a trained classifier, perhaps the 'best performer', to classify the USA dataset. \n",
    "            - Best accuracy may be to take a classifier that samples all 4 categories\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        if saved_classifier_name[-7:]=='untuned' or saved_classifier_name[-8-8:]=='untuned_balanced':\n",
    "            tuned = 'not_tuned'\n",
    "            print('not_tuned classifier')\n",
    "        else:\n",
    "            tuned = 'tuned'\n",
    "            print('classifier is tuned')\n",
    "        \n",
    "        \n",
    "        \n",
    "        for j, yc in enumerate(classLoopParams['year_classified']):\n",
    "            '''\n",
    "            Define the training image, then the image to classify, adding slope and elevation bands\n",
    "\n",
    "            Corine Representative Classes:\n",
    "            || 1989 -> 1998 | 1999 -> 2001 | 2005 -> 2007 | 2011 -> 2012 | 2017 -> 2018 ||\n",
    "            ||   \"2000\"     |    \"2000\"    |    \"2006\"    |     \"2012\"   |     \"2018\"   ||\n",
    "\n",
    "            Training \"image year above\" --> classify the relevant Landsat date range below:\n",
    "            || 1984 -> 1998 | 1999 -> 2003 | 2004 -> 2009 | 2010 -> 2014 | 2015 -> ...  || \n",
    "            \n",
    "            '''\n",
    "\n",
    "            if yc >= 1984 and yc < 2004:\n",
    "                classImageYear = 2000\n",
    "\n",
    "            elif yc >= 2004 and yc < 2010:\n",
    "                classImageYear = 2006\n",
    "\n",
    "            elif yc >= 2010 and yc < 2015:\n",
    "                classImageYear = 2012\n",
    "\n",
    "            elif yc >= 2015:\n",
    "                classImageYear = 2018\n",
    "\n",
    "            else:\n",
    "                print('ERROR: year to classify out of range[1984 +]')\n",
    "                break\n",
    "            \n",
    "            print('year classified', yc)\n",
    "            \n",
    "            #the image from the collection that we want to classify\n",
    "            imageFromCollection = ee.ImageCollection(annual_med_calc).filterDate(str(yc)+'-'+startDay, str(yc+1)+'-'+endDay).first()\n",
    "            \n",
    "            #image from training dataset e.g. CORINE is selected and simlified... \n",
    "            trainingClassImage = ltgee.createTrainingImage(str(classImageYear), \n",
    "                                                           classLoopParams['dataset'], \n",
    "                                                           classLoopParams['trainingClassLevel'], \n",
    "                                                           aoi)\n",
    "            \n",
    "            \n",
    "            #TODO: reproject to 30m native resolution\n",
    "            clipped = imageFromCollection.clip(aoi)\n",
    "            \n",
    "            #Adding the elevation and slope band calculations to each image\n",
    "            print('Adding Terrain...')\n",
    "            imageToClassify = ltgee.addTerrainBand(clipped, aoi)\n",
    "        \n",
    "            \n",
    "            #getting the date of the image and converting it from milliseconds since 1970 (Earth engines preferred datetime)\n",
    "            ms = msToDate(ee.Date(imageToClassify.get('system:time_start')).getInfo()['value'])\n",
    "            \n",
    "            print('Generating GCP...')\n",
    "            #the points used for training the classifier are randomly distibuted amongst the classes extracting a profile of spectral and terrain\n",
    "            points = ltgee.genGCP(trainingClassImage, \n",
    "                                  imageToClassify, \n",
    "                                  classLoopParams['numClasses'], #set at 5 usually\n",
    "                                  classLoopParams['split'],      #set at 70/30\n",
    "                                  classLoopParams['tileScale'], \n",
    "                                  aoi, \n",
    "                                  classLoopParams['distribution'], #either balanced or weighted\n",
    "                                  classLoopParams['weighting'])\n",
    "            \n",
    "            \n",
    "            # 70% of the points are allocated to training\n",
    "            training = points['training']\n",
    "            \n",
    "            # 30% of the points are allocated to classification\n",
    "            testing = points['testing']\n",
    "            \n",
    "            t5 = dt.today()\n",
    "            \n",
    "            print('Building classifier...')\n",
    "            \n",
    "            #classifier training with predefined number of trees using training points\n",
    "            classifier = ltgee.classifier(imageToClassify, training, tuned, classLoopParams['hyperparameters'])\n",
    "            \n",
    "            t6 = dt.today()\n",
    "            \n",
    "            print('saving the classifier..')\n",
    "            #saving decision trees for later use\n",
    "            saveClassifierToCSV(classifier, name, yc)\n",
    "            \n",
    "            \n",
    "            print('saved')\n",
    "            \n",
    "            '''\n",
    "            Accuracy metrics saved, firstly the testing metrics, generated via the confusion matrix, then similarly the Validation accuracy metrics (from the 30% split test). \n",
    "            A dataframe is generated, populated with the overall accuracy, kappa score, fscore, producers accuracy and consumers accuracy. \n",
    "            \n",
    "            The matrices (confusion/training & error/validation) are saved as individual npy files, and upon completion of a complete loop the list of all matrices are saved as a combined list of matrices. \n",
    "            \n",
    "            '''\n",
    "            \n",
    "            print('Asessing classifier accuracy...')\n",
    "            \n",
    "            confusion_matrix = classifier.confusionMatrix()\n",
    "            \n",
    "            cm = np.array(confusion_matrix.getInfo())[1:, 1:]\n",
    "            \n",
    "            consumers_accuracy, producers_accuracy, fscore, accuracy, kappa = calculate_classification_stats(cm)\n",
    "            \n",
    "            np.save(f'../Outputs/{dataset}/Trees/{saved_classifier_name}_{tuned}_confusionMatrix_{classImageYear}.npy', arr=cm, allow_pickle=True)   #saving the confusionMatrix to allow for later access in assessing the bias\n",
    "            \n",
    "            confusion_matrices.append(cm)\n",
    "\n",
    "            acc_dict = {'train_valid':'training',\n",
    "                        'tree_name':f'{saved_classifier_name}_{tuned}',\n",
    "                        'year':classImageYear, \n",
    "                        'o_accuracy':accuracy,\n",
    "                        'kappa':kappa,\n",
    "                        'fscore1':fscore[0],\n",
    "                        'fscore2':fscore[1],\n",
    "                        'fscore3':fscore[2],\n",
    "                        'fscore4':fscore[3],\n",
    "                        'fscore5':fscore[4],\n",
    "                        'p_accuracy1':producers_accuracy[0],\n",
    "                        'p_accuracy2':producers_accuracy[1],\n",
    "                        'p_accuracy3':producers_accuracy[2],\n",
    "                        'p_accuracy4':producers_accuracy[3],\n",
    "                        'p_accuracy5':producers_accuracy[4],\n",
    "                        'c_accuracy1':consumers_accuracy[0],\n",
    "                       'c_accuracy2':consumers_accuracy[1],\n",
    "                       'c_accuracy3':consumers_accuracy[2],\n",
    "                       'c_accuracy4':consumers_accuracy[3],\n",
    "                       'c_accuracy5':consumers_accuracy[4]}\n",
    "            \n",
    "            acc_df = pd.DataFrame(acc_dict, index=[0])\n",
    "            acc_df.to_excel(f'../Outputs/{dataset}/Trees/{saved_classifier_name}_{tuned}_accuracy_metrics_{classImageYear}.xlsx')\n",
    "            \n",
    "            importance = ee.Dictionary(classifier.explain()).get('importance').getInfo()\n",
    "            \n",
    "            validation_combined_df = validation_combined_df.append(acc_df)\n",
    "            \n",
    "            pd.DataFrame(importance, index=[0]).to_excel(f'../Outputs/{dataset}/Trees/{saved_classifier_name}_{tuned}_importance_{classImageYear}.xlsx')\n",
    "            \n",
    "            '''\n",
    "            Validation metrics saved\n",
    "            '''\n",
    "            \n",
    "            print('Asessing classifier validation accuracy...')\n",
    "            \n",
    "            validated = testing.classify(classifier)\n",
    "            \n",
    "            test_matrix = validated.errorMatrix('landcover', 'classification')\n",
    "            \n",
    "            em = np.array(test_matrix.getInfo())[1:,1:]\n",
    "            \n",
    "            consumers_accuracy, producers_accuracy, fscore, accuracy, kappa = calculate_classification_stats(em)\n",
    "            \n",
    "            val_dict = {'train_valid':'validation',\n",
    "                        'tree_name':f'{saved_classifier_name}_{tuned}',\n",
    "                        'year':classImageYear, \n",
    "                        'o_accuracy':accuracy,\n",
    "                        'kappa':kappa,\n",
    "                        'fscore1':fscore[0],\n",
    "                        'fscore2':fscore[1],\n",
    "                        'fscore3':fscore[2],\n",
    "                        'fscore4':fscore[3],\n",
    "                        'fscore5':fscore[4],\n",
    "                        'p_accuracy1':producers_accuracy[0],\n",
    "                        'p_accuracy2':producers_accuracy[1],\n",
    "                        'p_accuracy3':producers_accuracy[2],\n",
    "                        'p_accuracy4':producers_accuracy[3],\n",
    "                        'p_accuracy5':producers_accuracy[4],\n",
    "                        'c_accuracy1':consumers_accuracy[0],\n",
    "                       'c_accuracy2':consumers_accuracy[1],\n",
    "                       'c_accuracy3':consumers_accuracy[2],\n",
    "                       'c_accuracy4':consumers_accuracy[3],\n",
    "                       'c_accuracy5':consumers_accuracy[4]}\n",
    "                        \n",
    "            val_df = pd.DataFrame(val_dict, index=[0])\n",
    "            \n",
    "            val_df.to_excel(f'../Outputs/{dataset}/Trees/{saved_classifier_name}_{tuned}_validation_metrics_{classImageYear}.xlsx')\n",
    "            \n",
    "            np.save(f'../Outputs/{dataset}/Trees/{saved_classifier_name}_{tuned}_validationMatrix_{classImageYear}.npy', arr=em, allow_pickle=True)   #saving the confusionMatrix to allow for later access in assessing the bias\n",
    "            \n",
    "            print('Classifying...')\n",
    "            \n",
    "            classified = imageToClassify.classify(classifier)\n",
    "            \n",
    "            validation_combined_df = validation_combined_df.append(val_df)\n",
    "            \n",
    "            t2 = dt.today()\n",
    "            \n",
    "            print(f'time for {classImageYear} data = {t2 - t3}')\n",
    "            \n",
    "    else:\n",
    "        print('classification routine for this dataset is not yet provided for')\n",
    "    \n",
    "    t4 = dt.today()\n",
    "    print(f'step2: Done: {t4}, time taken: {t4-t3}')\n",
    "    print(f'\\nCatchment: {name}, total time: {t4-t1}\\n---------------')\n",
    "    \n",
    "    if ind == sys_index[0]:\n",
    "        break\n",
    "\n",
    "classArea_df.to_excel(f'../Outputs/{dataset}/RF_classification/{saved_classifier_name}_{tuned}_combinedAreas.xlsx')\n",
    "\n",
    "np.save(f'../Outputs/{dataset}/Trees/{saved_classifier_name}_{tuned}_confusionMatrices_All_{classImageYear}.npy', arr=confusion_matrices, allow_pickle=True)  \n",
    "\n",
    "np.save(f'../Outputs/{dataset}/Trees/{saved_classifier_name}_{tuned}_errorMatrices_All_{classImageYear}.npy', arr=error_matrices, allow_pickle=True)  \n",
    "\n",
    "\n",
    "tfinal = dt.today()\n",
    "\n",
    "print(f'END LOOP: Full routine finished: {tfinal} \\nTime taken: {tfinal-t0}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
