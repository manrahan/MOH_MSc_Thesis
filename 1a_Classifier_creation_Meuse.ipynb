{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb0faf5-3525-4b3d-bab4-5044b92b2513",
   "metadata": {},
   "source": [
    "# Training a Random Forest Model over a CORINE Dataset using Google Earth Engine with the Python API \n",
    "\n",
    "This notebook demonstrates how to train a random forest model over a CORINE dataset using Google Earth Engine with the Python API. The goal is to classify land cover categories based on the CORINE data and create decision trees using the random forest algorithm. We will also use geemap to visualize the results and save the decision trees locally.\n",
    "\n",
    "The workflow consists of the following steps:\n",
    "\n",
    "1. Set up the environment by importing the necessary libraries and authenticating our Earth Engine account.\n",
    "\n",
    "2. Defining the study area via GEE asset (table or shapefile)\n",
    "\n",
    "3. Looping through the years, training over the relevant Corine data\n",
    "\n",
    "4. Returning the created decision trees with accuracy assesment detail. \n",
    "\n",
    "Saving the classifier will allow to classify a more continuous timeseries. \n",
    "\n",
    "## Useful to know\n",
    "\n",
    "- Here the classifiers are saved locally for re-use. Unfortunately due to server limits, a trained model cannot exceed 10MB before memory limits are encountered. \n",
    "\n",
    "- The underlying landsat timeseries is created using an adaptation of the landTrendr module, adapted and simplified here for efficiency. It accomplishes, creating the image series and band calculation. This version uses the latest processing efforts of the Landsat TM+ ETM+ and OLI collection 2\n",
    "\n",
    "\n",
    "The classification routine has been implemented here by Mike O'Hanrahan for their MSc graduation thesis project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35c5da59-29d8-4d04-b844-6c654540831f",
   "metadata": {
    "id": "e6157fac-15c5-4016-b24e-3729960e6844"
   },
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import geemap.ml as ml\n",
    "from ipygee import chart as chart\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "400e2bed-a901-45ad-bbe0-662c3eb4d019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is:  2023-04-18 14:38:47.002324\n"
     ]
    }
   ],
   "source": [
    "today = dt.today()\n",
    "print(\"Today is: \", today)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb97ec-c96b-4312-9908-67f52483ec8f",
   "metadata": {},
   "source": [
    "## GEE Authentication\n",
    "\n",
    "Before using the Earth Engine Python API, we need to authenticate our account. The authentication step is required for the first time you use Earth Engine in a new session and roughly every week thereafter.\n",
    "\n",
    "To authenticate, run the following cell and follow the prompts to log into your Earth Engine account. You will then be prompted to copy and paste the authentication code into the box provided. Once you have pasted the code, press enter to save the token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c4ac507-28df-4313-a67c-2db9fb8681ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ee.Authenticate()\n",
    "geemap.ee_initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5173d2-6427-4ac7-92c7-8230d1fe0ee1",
   "metadata": {},
   "source": [
    "## Hydroclimatic Information\n",
    "\n",
    "The code below defines a list of catchment names for Belgium and France by reading Excel files and concatenating them together. It then defines a function to remove whitespace at the end of each catchment name. Finally, it applies this function to each catchment name in the list, and prints out the total number of catchments and the resulting list of cleaned catchment names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dce165c-0e65-44be-b541-994dd9b4a4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 catchments processed for hydroclimatic variables:\n",
      " \n",
      "['Membre Pont', 'Straimont', 'Treignes', 'Chooz', 'Daverdisse', 'Jemelle', 'Hastiere', 'Warnant', 'Ortho', 'Wiheries', 'Salzinnes', 'Huccorgne', 'Amay', 'La Meuse Goncourt', 'Le Mouzon Circourt-sur-Mouzon [Villars]', 'Le Vair Soulosse-sous-Saint-Élophe', 'La Meuse Saint-Mihiel', 'La Meuse Stenay', 'La Chiers Longlaville', 'La Chiers Carignan', 'La Bar Cheveuges', 'La Vence la Francheville']\n"
     ]
    }
   ],
   "source": [
    "p = '..'\n",
    "\n",
    "l_BE = pd.read_excel(f\"{p}\\Inputs\\Version_3_20230303\\BE.xlsx\").catchment.tolist()\n",
    "l_FR = pd.read_excel(f\"{p}\\Inputs\\Version_3_20230303\\FR.xlsx\").catchment.tolist()\n",
    "\n",
    "ls = l_BE+l_FR\n",
    "\n",
    "def drop_space(i) -> str:\n",
    "    '''\n",
    "    Since the FR and BE data is given with indices using catchment names,\n",
    "    it is necessary to check and drop the space at the end of each name where applicable.\n",
    "    \n",
    "    list the letters, check the last charachter and delete if necessary. \n",
    "    '''\n",
    "    \n",
    "    ls = list(i)\n",
    "    \n",
    "    if ls[-1] == ' ':\n",
    "        i = i[:-1]\n",
    "    else:\n",
    "        i = i\n",
    "    return i\n",
    "\n",
    "\n",
    "names = [drop_space(i) for i in ls]\n",
    "\n",
    "print(f'{len(names)} catchments processed for hydroclimatic variables:\\n \\n{names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18be9e09-35db-4adb-92c7-5ebc78e231da",
   "metadata": {},
   "source": [
    "## Load the EE package\n",
    "\n",
    "This notebook uses an adapted landTrendr package to construct time series of Landsat imagery for land cover detection. The package is optimized for deforestation event detection and can be used with the latest version of landTrendr available in the GEE asset, which has an Apache license and is free to use.\n",
    "\n",
    "To load the package, we use the ltgee.buildSRcollection method. Note that if the JavaScript module is faulty, the cell below will not load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c702bf6-33c6-4c28-ba17-2eb15870d719",
   "metadata": {
    "id": "e6157fac-15c5-4016-b24e-3729960e6844"
   },
   "outputs": [],
   "source": [
    "oeel = geemap.requireJS()\n",
    "\n",
    "Map = geemap.Map()\n",
    "\n",
    "ltgee = geemap.requireJS(r'../JS_module/Adapted_LT_v6.js')\n",
    "\n",
    "#ltgee.availability  #all functions within the javascript module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe87d20-395d-4fe1-8b9c-75e96c9419f0",
   "metadata": {},
   "source": [
    "## Initiate With a Shapefile\n",
    "\n",
    "This notebook assumes the user has a shapefile saved as an asset on their GEE, the assets used in the CATAPUCII project will be made publicly available in the @mohanrahan repository\n",
    "\n",
    "Assigning some useful variables for later classification\n",
    "\n",
    "Catchment Assets are available at this address:\n",
    "\n",
    "https://code.earthengine.google.com/?asset=projects/mohanrahan/assets/CATAPUCII_Catchments\n",
    "\n",
    "\n",
    "## Assigning useful variables\n",
    "\n",
    "- The asset_dir will point to the shapefile loaded as a GEE table asset. \n",
    "- crs is important for reprojection and scaling (which will affect area calculations)\n",
    "- RGB_VIS is for landsat RGB visual parameters\n",
    "- start day defines the beginning of the seasonal composite period\n",
    "- maskThese applies a mask (renders Null/NA/Transparent) to those majority pixels in landsat imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9045fa07-c860-4501-a0a3-77c980967b84",
   "metadata": {
    "id": "103ce65e-6dc2-4309-9cc8-e7374bf57af3"
   },
   "outputs": [],
   "source": [
    "asset_dir = 'CATAPUCII_Catchments/Meuse_Catchments_4326_WFLOW'\n",
    "\n",
    "dataset = 'Meuse'\n",
    "\n",
    "crs = 'EPSG:4326'\n",
    "\n",
    "fignum = 0\n",
    "\n",
    "RGB_VIS = {'bands':['B3','B2','B1'], 'min':0, 'max':1.5e3}\n",
    "\n",
    "startYear = 1984\n",
    "\n",
    "endYear = 2022\n",
    "\n",
    "startDay = '06-20'\n",
    "\n",
    "endDay = '08-31'\n",
    "\n",
    "maskThese = ['cloud', 'shadow', 'snow',]\n",
    "\n",
    "bandList = [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B7\", \n",
    "           'NBR', 'NDMI', 'NDVI', 'NDSI', 'EVI','GNDVI', \n",
    "           'TCB', 'TCG', 'TCW', 'TCA', 'NDFI',] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24017ee3-b207-447c-938e-200892cfed46",
   "metadata": {},
   "source": [
    "## Load and filter the table data \n",
    "\n",
    "- The code loads a FeatureCollection from a Google Earth Engine asset directory, using the ee.FeatureCollection method.\n",
    "- It defines three functions to calculate the area of each geometry in square kilometer, set the system ID as a column, and calculate the pixel area.\n",
    "- It applies these functions to the FeatureCollection using the map method, and saves the resulting FeatureCollection as table_area.\n",
    "- It filters and sorts table_area to obtain only those features whose area_km2 is greater than 0 and sorts them by area_km2.\n",
    "- It converts the resulting FeatureCollection to a pandas DataFrame using geemap.ee_to_pandas and sets the index to system_index.\n",
    "- It selects the rows of the DataFrame whose station_re is in the names list and saves the resulting DataFrame as df1.\n",
    "- It saves the entire DataFrame as an Excel file in a specified output directory, using the to_excel method.\n",
    "\n",
    "\n",
    "Overall, the code processes a FeatureCollection from a Google Earth Engine asset directory, calculates various areas, filters and sorts the features, selects a subset of features based on a list of catchment names, and saves the resulting DataFrame as an Excel file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "257eab36-74c7-46b1-97e9-3cb1af199dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_area</th>\n",
       "      <th>area_km2</th>\n",
       "      <th>station_Y_</th>\n",
       "      <th>wflowID</th>\n",
       "      <th>station_X_</th>\n",
       "      <th>station_re</th>\n",
       "      <th>station_na</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>system_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00000000000000000004</th>\n",
       "      <td>2681.514071</td>\n",
       "      <td>2672.267778</td>\n",
       "      <td>50.466667</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.835833</td>\n",
       "      <td>Salzinnes</td>\n",
       "      <td>Salzinnes Ronet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000000001</th>\n",
       "      <td>1471.943902</td>\n",
       "      <td>1467.021734</td>\n",
       "      <td>50.091667</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.810833</td>\n",
       "      <td>Chooz</td>\n",
       "      <td>Chooz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000000000</th>\n",
       "      <td>1368.127798</td>\n",
       "      <td>1363.749108</td>\n",
       "      <td>49.491667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.177500</td>\n",
       "      <td>La Meuse Stenay</td>\n",
       "      <td>La Meuse Stenay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000000000d</th>\n",
       "      <td>1349.324088</td>\n",
       "      <td>1345.191904</td>\n",
       "      <td>48.866667</td>\n",
       "      <td>101.0</td>\n",
       "      <td>5.527500</td>\n",
       "      <td>La Meuse Saint-Mihiel</td>\n",
       "      <td>La Meuse Saint-Mihiel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000000022</th>\n",
       "      <td>1247.982599</td>\n",
       "      <td>1243.674956</td>\n",
       "      <td>50.533333</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>5.319167</td>\n",
       "      <td>Amay</td>\n",
       "      <td>Amay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000000000e</th>\n",
       "      <td>1067.196654</td>\n",
       "      <td>1063.732496</td>\n",
       "      <td>49.633333</td>\n",
       "      <td>201.0</td>\n",
       "      <td>5.144167</td>\n",
       "      <td>La Chiers Carignan</td>\n",
       "      <td>La Chiers Carignan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000000002</th>\n",
       "      <td>913.747728</td>\n",
       "      <td>910.700030</td>\n",
       "      <td>49.866667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.902500</td>\n",
       "      <td>Membre Pont</td>\n",
       "      <td>Membre Pont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000000003</th>\n",
       "      <td>550.486854</td>\n",
       "      <td>548.630959</td>\n",
       "      <td>50.091667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.677500</td>\n",
       "      <td>Treignes</td>\n",
       "      <td>Treignes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000000021</th>\n",
       "      <td>440.093432</td>\n",
       "      <td>438.791674</td>\n",
       "      <td>48.400000</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>5.735833</td>\n",
       "      <td>Le Vair Soulosse-sous-Saint-Élophe</td>\n",
       "      <td>Le Vair Soulosse-sous-Saint-Élophe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000000001b</th>\n",
       "      <td>417.977178</td>\n",
       "      <td>416.552196</td>\n",
       "      <td>50.158333</td>\n",
       "      <td>803.0</td>\n",
       "      <td>5.260833</td>\n",
       "      <td>Jemelle</td>\n",
       "      <td>Jemelle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000000000a</th>\n",
       "      <td>406.385120</td>\n",
       "      <td>405.064080</td>\n",
       "      <td>49.658333</td>\n",
       "      <td>41.0</td>\n",
       "      <td>4.885833</td>\n",
       "      <td>La Bar Cheveuges</td>\n",
       "      <td>La Bar Cheveuges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000000020</th>\n",
       "      <td>405.463720</td>\n",
       "      <td>404.259759</td>\n",
       "      <td>48.316667</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>5.710833</td>\n",
       "      <td>Le Mouzon Circourt-sur-Mouzon [Villars]</td>\n",
       "      <td>Le Mouzon Circourt-sur-Mouzon [Villars]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000000001d</th>\n",
       "      <td>387.477644</td>\n",
       "      <td>386.163237</td>\n",
       "      <td>50.100000</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>5.644167</td>\n",
       "      <td>Ortho</td>\n",
       "      <td>Ortho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000000001f</th>\n",
       "      <td>368.160671</td>\n",
       "      <td>367.091382</td>\n",
       "      <td>48.241667</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>5.610833</td>\n",
       "      <td>La Meuse Goncourt</td>\n",
       "      <td>La Meuse Goncourt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000000008</th>\n",
       "      <td>305.818286</td>\n",
       "      <td>304.731203</td>\n",
       "      <td>50.566667</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.169167</td>\n",
       "      <td>Huccorgne</td>\n",
       "      <td>Huccorgne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000000001a</th>\n",
       "      <td>302.637555</td>\n",
       "      <td>301.614998</td>\n",
       "      <td>50.041667</td>\n",
       "      <td>802.0</td>\n",
       "      <td>5.127500</td>\n",
       "      <td>Daverdisse</td>\n",
       "      <td>Daverdisse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000000013</th>\n",
       "      <td>189.952638</td>\n",
       "      <td>189.318019</td>\n",
       "      <td>49.791667</td>\n",
       "      <td>501.0</td>\n",
       "      <td>5.385833</td>\n",
       "      <td>Straimont</td>\n",
       "      <td>Straimont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000000015</th>\n",
       "      <td>162.777877</td>\n",
       "      <td>162.233786</td>\n",
       "      <td>50.208333</td>\n",
       "      <td>701.0</td>\n",
       "      <td>4.794167</td>\n",
       "      <td>Hastiere</td>\n",
       "      <td>Hastiere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000000000f</th>\n",
       "      <td>152.489243</td>\n",
       "      <td>151.982904</td>\n",
       "      <td>49.533333</td>\n",
       "      <td>203.0</td>\n",
       "      <td>5.802500</td>\n",
       "      <td>La Chiers Longlaville</td>\n",
       "      <td>La Chiers Longlaville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000000001c</th>\n",
       "      <td>142.240404</td>\n",
       "      <td>141.760418</td>\n",
       "      <td>50.300000</td>\n",
       "      <td>903.0</td>\n",
       "      <td>4.177500</td>\n",
       "      <td>Wiheries</td>\n",
       "      <td>Wiheries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000000000b</th>\n",
       "      <td>124.878635</td>\n",
       "      <td>124.471389</td>\n",
       "      <td>49.725000</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4.710833</td>\n",
       "      <td>La Vence la Francheville</td>\n",
       "      <td>La Vence la Francheville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000000000017</th>\n",
       "      <td>123.921347</td>\n",
       "      <td>123.473771</td>\n",
       "      <td>50.308333</td>\n",
       "      <td>703.0</td>\n",
       "      <td>4.827500</td>\n",
       "      <td>Warnant</td>\n",
       "      <td>Warnant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       pixel_area     area_km2  station_Y_  wflowID  \\\n",
       "system_index                                                          \n",
       "00000000000000000004  2681.514071  2672.267778   50.466667      9.0   \n",
       "00000000000000000001  1471.943902  1467.021734   50.091667      4.0   \n",
       "00000000000000000000  1368.127798  1363.749108   49.491667      3.0   \n",
       "0000000000000000000d  1349.324088  1345.191904   48.866667    101.0   \n",
       "00000000000000000022  1247.982599  1243.674956   50.533333   1401.0   \n",
       "0000000000000000000e  1067.196654  1063.732496   49.633333    201.0   \n",
       "00000000000000000002   913.747728   910.700030   49.866667      5.0   \n",
       "00000000000000000003   550.486854   548.630959   50.091667      6.0   \n",
       "00000000000000000021   440.093432   438.791674   48.400000   1016.0   \n",
       "0000000000000000001b   417.977178   416.552196   50.158333    803.0   \n",
       "0000000000000000000a   406.385120   405.064080   49.658333     41.0   \n",
       "00000000000000000020   405.463720   404.259759   48.316667   1013.0   \n",
       "0000000000000000001d   387.477644   386.163237   50.100000   1002.0   \n",
       "0000000000000000001f   368.160671   367.091382   48.241667   1011.0   \n",
       "00000000000000000008   305.818286   304.731203   50.566667     13.0   \n",
       "0000000000000000001a   302.637555   301.614998   50.041667    802.0   \n",
       "00000000000000000013   189.952638   189.318019   49.791667    501.0   \n",
       "00000000000000000015   162.777877   162.233786   50.208333    701.0   \n",
       "0000000000000000000f   152.489243   151.982904   49.533333    203.0   \n",
       "0000000000000000001c   142.240404   141.760418   50.300000    903.0   \n",
       "0000000000000000000b   124.878635   124.471389   49.725000     42.0   \n",
       "00000000000000000017   123.921347   123.473771   50.308333    703.0   \n",
       "\n",
       "                      station_X_                               station_re  \\\n",
       "system_index                                                                \n",
       "00000000000000000004    4.835833                                Salzinnes   \n",
       "00000000000000000001    4.810833                                    Chooz   \n",
       "00000000000000000000    5.177500                          La Meuse Stenay   \n",
       "0000000000000000000d    5.527500                    La Meuse Saint-Mihiel   \n",
       "00000000000000000022    5.319167                                     Amay   \n",
       "0000000000000000000e    5.144167                       La Chiers Carignan   \n",
       "00000000000000000002    4.902500                              Membre Pont   \n",
       "00000000000000000003    4.677500                                 Treignes   \n",
       "00000000000000000021    5.735833       Le Vair Soulosse-sous-Saint-Élophe   \n",
       "0000000000000000001b    5.260833                                  Jemelle   \n",
       "0000000000000000000a    4.885833                         La Bar Cheveuges   \n",
       "00000000000000000020    5.710833  Le Mouzon Circourt-sur-Mouzon [Villars]   \n",
       "0000000000000000001d    5.644167                                    Ortho   \n",
       "0000000000000000001f    5.610833                        La Meuse Goncourt   \n",
       "00000000000000000008    5.169167                                Huccorgne   \n",
       "0000000000000000001a    5.127500                               Daverdisse   \n",
       "00000000000000000013    5.385833                                Straimont   \n",
       "00000000000000000015    4.794167                                 Hastiere   \n",
       "0000000000000000000f    5.802500                    La Chiers Longlaville   \n",
       "0000000000000000001c    4.177500                                 Wiheries   \n",
       "0000000000000000000b    4.710833                 La Vence la Francheville   \n",
       "00000000000000000017    4.827500                                  Warnant   \n",
       "\n",
       "                                                   station_na  \n",
       "system_index                                                   \n",
       "00000000000000000004                          Salzinnes Ronet  \n",
       "00000000000000000001                                    Chooz  \n",
       "00000000000000000000                          La Meuse Stenay  \n",
       "0000000000000000000d                    La Meuse Saint-Mihiel  \n",
       "00000000000000000022                                     Amay  \n",
       "0000000000000000000e                       La Chiers Carignan  \n",
       "00000000000000000002                              Membre Pont  \n",
       "00000000000000000003                                 Treignes  \n",
       "00000000000000000021       Le Vair Soulosse-sous-Saint-Élophe  \n",
       "0000000000000000001b                                  Jemelle  \n",
       "0000000000000000000a                         La Bar Cheveuges  \n",
       "00000000000000000020  Le Mouzon Circourt-sur-Mouzon [Villars]  \n",
       "0000000000000000001d                                    Ortho  \n",
       "0000000000000000001f                        La Meuse Goncourt  \n",
       "00000000000000000008                                Huccorgne  \n",
       "0000000000000000001a                               Daverdisse  \n",
       "00000000000000000013                                Straimont  \n",
       "00000000000000000015                                 Hastiere  \n",
       "0000000000000000000f                    La Chiers Longlaville  \n",
       "0000000000000000001c                                 Wiheries  \n",
       "0000000000000000000b                 La Vence la Francheville  \n",
       "00000000000000000017                                  Warnant  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "table = ee.FeatureCollection(f\"projects/mohanrahan/assets/{asset_dir}\")\n",
    "\n",
    "def set_area_km2(feature):\n",
    "    '''\n",
    "    Calculate the area of each geometry in square kilometer\n",
    "    '''\n",
    "    area = feature.geometry().area().divide(1000*1000)\n",
    "    setting = feature.set('area_km2', area)\n",
    "    return setting\n",
    "\n",
    "def set_area_pixel(feature):\n",
    "    aoi = feature.geometry()\n",
    "    area = ee.Image.pixelArea().divide(1e6).clip(aoi).select('area').reduceRegion(**{\n",
    "        'reducer':ee.Reducer.sum(),\n",
    "        'geometry':aoi,\n",
    "        'scale':30,\n",
    "        'crs':'EPSG:4326',\n",
    "        'maxPixels':1e13,\n",
    "        'bestEffort':True,\n",
    "        }).get('area')\n",
    "    setting = feature.set('pixel_area', area)\n",
    "    return setting\n",
    "\n",
    "def set_id(feature):\n",
    "    '''\n",
    "    Set the system ID as a column\n",
    "    '''\n",
    "    getting_name = ee.String(feature.get('system:index'))\n",
    "    setting_id = feature.set({'system_index':getting_name,})\n",
    "    return setting_id\n",
    "\n",
    "table_area = table.map(set_area_km2).map(set_id).map(set_area_pixel)\n",
    "\n",
    "Filtered_Sorted = table_area.filter(ee.Filter.gt('area_km2', 0)).sort('area_km2', False)  # true ranks from smallest to largest\n",
    "\n",
    "down = geemap.ee_to_pandas(Filtered_Sorted).set_index(['system_index'])\n",
    "\n",
    "df1 = down.loc[down['station_re'].isin(names)]\n",
    "\n",
    "sys_index = df1.index.to_list()\n",
    "\n",
    "display(df1)\n",
    "\n",
    "df1.loc[sys_index[-1]]\n",
    "\n",
    "print(len(df1))\n",
    "\n",
    "\n",
    "gdf = geemap.ee_to_pandas(Filtered_Sorted)\n",
    "\n",
    "if not os.path.exists(f'../Outputs/{dataset}/'):\n",
    "    os.makedirs(f'../Outputs/{dataset}/')\n",
    "\n",
    "gdf.to_excel(f'../Outputs/{dataset}/{dataset}_catchment_table.xlsx', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cec8f4c-3729-462e-94e8-78020f018376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d9cbe8befb4da9b2932e8e645aa058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[50.15762767624681, 5.442982293509325], controls=(ZoomControl(options=['position', 'zoom_in_text', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Map = geemap.Map()\n",
    "\n",
    "Map.setOptions('TERRAIN')\n",
    "Map.addLayer(Filtered_Sorted.filter(ee.Filter.inList('station_re', ee.List(names))), {'color': 'green'}, 'green: Included')\n",
    "Map.addLayer(Filtered_Sorted.filter(ee.Filter.inList('station_re', ee.List(names)).Not()), {'color':'red'}, 'red: Not Included')\n",
    "Map.centerObject(Filtered_Sorted, 7)\n",
    "\n",
    "\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fec43c4c-115d-4483-bb47-8c8994832a42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clip_collection(image: ee.Image) -> ee.Image:\n",
    "    '''\n",
    "    Clip an image to the area of interest and return a copy of the clipped image.\n",
    "\n",
    "    Input:\n",
    "        image (ee.Image): the image to clip\n",
    "    \n",
    "    Output:\n",
    "        ee.Image: the clipped image\n",
    "    '''\n",
    "    return image.clip(aoi).copyProperties(image)\n",
    "\n",
    "\n",
    "def extractArea(item) -> ee.List:\n",
    "    '''\n",
    "    Extract the area of a classified land cover class as a list of class number and area.\n",
    "\n",
    "    Input:\n",
    "        item (ee.Dictionary): a dictionary containing classification and pixel area information\n",
    "    \n",
    "    Output:\n",
    "        ee.List: a list of class number and area\n",
    "    '''\n",
    "    \n",
    "    areaDict = ee.Dictionary(item)\n",
    "    classNumber = ee.Number(areaDict.get('classification')).format()\n",
    "    area = ee.Number(areaDict.get('sum')).divide(1e6)\n",
    "    return ee.List([classNumber, area])\n",
    "\n",
    "\n",
    "def classArea(classified_image: ee.Image, scale: int) -> ee.List:\n",
    "    '''\n",
    "    Calculate the area of each classified land cover class for an image at a given scale.\n",
    "\n",
    "    Input:\n",
    "        classified_image (ee.Image): classified image\n",
    "        scale (int): scale of the image in meters\n",
    "    \n",
    "    Output:\n",
    "        ee.List: a list of class numbers and areas for the image\n",
    "    '''\n",
    "    \n",
    "    areaImage = ee.Image.pixelArea().addBands(classified_image)\n",
    "    \n",
    "    areas = areaImage.reduceRegion(**{\n",
    "            'reducer':ee.Reducer.sum().group(**{'groupField':1, 'groupName':'classification'}),\n",
    "            'geometry':aoi,\n",
    "            'scale':scale,\n",
    "            'maxPixels':1e10,\n",
    "            'bestEffort':True,\n",
    "    })\n",
    "    \n",
    "    classAreas = ee.List(areas.get('groups'))\n",
    "    \n",
    "    classAreasLists = classAreas.map(extractArea)\n",
    "    \n",
    "    return classAreasLists\n",
    "\n",
    "def msToDate(milliseconds: int) -> datetime.datetime:\n",
    "    '''\n",
    "    Convert a timestamp in milliseconds to a datetime object.\n",
    "\n",
    "    Input:\n",
    "        milliseconds (int): a timestamp in milliseconds\n",
    "    \n",
    "    Output:\n",
    "        datetime.datetime: a datetime object corresponding to the timestamp\n",
    "    '''\n",
    "    \n",
    "    base_datetime = datetime.datetime(1970, 1, 1)\n",
    "    delta = datetime.timedelta(0, 0, 0, milliseconds)\n",
    "    target_datetime = base_datetime + delta\n",
    "    return target_datetime\n",
    "\n",
    "def dataframeAreas(i: int, yc: int, classified: ee.Image, trainingClassImage: ee.Image, ms: int, classImageYear: int, name: str, accuracy: float) -> pd.DataFrame:\n",
    "    '''\n",
    "    Calculate the area of each classified land cover class for an image, given its classification and training images, and return a DataFrame of the results.\n",
    "\n",
    "    Input:\n",
    "        i (int): index of the current image in the collection\n",
    "        yc (int): year count (number of years since the first year in the collection)\n",
    "        classified (ee.Image): classified image\n",
    "        trainingClassImage (ee.Image): training image with land cover classes\n",
    "        ms (int): timestamp of the image in milliseconds\n",
    "        classImageYear (int): year of the image used for classification\n",
    "        name (str): name of the catchment\n",
    "        accuracy (float): accuracy of the classifier used for classification\n",
    "    \n",
    "    Output:\n",
    "        pivoted (pd.DataFrame): a pivoted DataFrame with columns representing land cover classes and rows representing image dates, area of each class for CORINE and random forest classifications, and catchment name, training year, and accuracy information\n",
    "    '''\n",
    "    \n",
    "    ls1 = pd.DataFrame(classArea(classified, 30).getInfo(), columns=['class', 'area_RF'])\n",
    "    ls2 = pd.DataFrame(classArea(trainingClassImage, 100).getInfo(), columns=['class', 'area_CORINE'])\n",
    "\n",
    "    merged = ls1.merge(ls2, how='inner', on='class')\n",
    "    merged['image_date'] = ms\n",
    "    pivoted = merged.pivot(index='image_date', columns='class', values=['area_CORINE', 'area_RF'])\n",
    "    pivoted['training', 'year_trained'] =  classImageYear\n",
    "    pivoted['area_CORINE', '6'] = 0\n",
    "    pivoted['catchment', 'area'] = pivoted.iloc[0, 0:5].sum()\n",
    "    pivoted['area_RF', '6'] = pivoted.catchment.area - pivoted.iloc[0, 6:10].sum() \n",
    "    pivoted['catchment', 'name '] = name\n",
    "    pivoted['testing', 'accuracy'] = accuracy\n",
    "    pivoted['ind'] = str(i)+'_'+str(yc)\n",
    "    pivoted.fillna(0)\n",
    "    \n",
    "    return pivoted\n",
    "\n",
    "\n",
    "    \n",
    "def saveClassifierToCSV(classifier: ee.Classifier, name: str, yc: int) -> None:\n",
    "    '''\n",
    "    Save decision trees from a random forest classifier to CSV files.\n",
    "\n",
    "    Input:\n",
    "        classifier (ee.Classifier): a trained random forest classifier\n",
    "        name (str): name of the catchment\n",
    "        yc (int): year count (number of years since the first year in the collection)\n",
    "    \n",
    "    Output:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    decisionTrees = ee.List(classifier.explain().get('trees')).getInfo()\n",
    "    folder='Trees'\n",
    "    \n",
    "    print('saving...')\n",
    "\n",
    "    var = f'../Outputs/{dataset}/{folder}/'\n",
    "\n",
    "    if not os.path.exists(var):\n",
    "        print('created')\n",
    "        os.makedirs(var)\n",
    "\n",
    "    ml.trees_to_csv(decisionTrees, f'../Outputs/Meuse/Trees/{name}_{yc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0209bb49-7f47-40c1-aba7-280cbd0e3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ls = 'used_images'\n",
    "SR_t = 'SR_timeseries'\n",
    "RF_c = 'RF_classification/'\n",
    "\n",
    "\n",
    "folder_list = [id_ls, SR_t, RF_c]\n",
    "\n",
    "for folder in folder_list:\n",
    "    \n",
    "    '''\n",
    "    Create output folders if they don't exist.\n",
    "\n",
    "    Input:\n",
    "        folder (str): the name of the folder to create\n",
    "    \n",
    "    Output:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    var = f'../Outputs/{dataset}/{folder}'\n",
    "    \n",
    "    if not os.path.exists(var):\n",
    "        print('created')\n",
    "        os.makedirs(var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86deecbc-4c78-450a-b6d0-241bbd355d8f",
   "metadata": {},
   "source": [
    "\n",
    "### areas calculated from: \n",
    "\n",
    "https://code.earthengine.google.com/cc1f2371404daf318b08060def5a1aa5\n",
    "\n",
    "- This code defines a list of land cover class names and three dictionaries of land cover class areas in square kilometers for different regions of interest. \n",
    "- It then extracts the trained area in square kilometers from the area_for_weighted_sample dictionary and calculates the total. \n",
    "- It calculates the percentage of each land cover class within the total trained area and a weight for each land cover class based on its percentage of the total trained area. \n",
    "- Finally, it prints the weights for each land cover class to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce65a098-6d04-4387-b482-866bf1419ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points from percentage area:\n",
      "[('Artificial', 614), ('Agricultural', 2704), ('Forest and Semi-Natural', 1637), ('Wetlands', 11), ('Waterbodies', 30)]\n",
      "[0.12299685 0.54094232 0.32752478 0.00237046 0.0061656 ]\n"
     ]
    }
   ],
   "source": [
    "# Define a list of land cover class names\n",
    "class_names = ['Artificial', 'Agricultural', 'Forest and Semi-Natural', 'Wetlands', 'Waterbodies']\n",
    "\n",
    "# Define dictionaries of land cover class areas in square kilometers for different regions of interest\n",
    "area_22_catchment = {\"1\": 1272.5302137428105, \"2\": 7980.776893726768, \"3\": 5614.751890457247, \"4\": 8.40213411897978, \"5\": 53.69054158948903}\n",
    "\n",
    "# The area as calculated from the full basin including the catchments outside of the classification area\n",
    "meuse_area_full_basin = {\"1\": 3515.899739181923, \"2\": 15462.989230419153, \"3\": 9362.388401473556, \"4\": 67.76015865491058, \"5\": 176.2453267149072}\n",
    "area_for_weighted_sample = meuse_area_full_basin\n",
    "\n",
    "# Extract the trained area in square kilometers from the area_for_weighted_sample dictionary and calculate the total\n",
    "trained_area_km = [area_for_weighted_sample[i] for i in area_for_weighted_sample.keys()]\n",
    "tot = np.sum(trained_area_km)\n",
    "\n",
    "# Calculate the percentage of each land cover class within the total trained area\n",
    "area_full_pct = np.array([area_for_weighted_sample[i] / tot for i in area_for_weighted_sample.keys()])\n",
    "\n",
    "# Calculate a weight for each land cover class based on its percentage of the total trained area\n",
    "weights = [int(p * 5000) for p in area_full_pct]\n",
    "\n",
    "# Print the weights for each land cover class to the console\n",
    "weight_zip = zip(class_names, weights)\n",
    "print(f'Points from percentage area:\\n{[(cla, point) for cla, point in weight_zip]}')\n",
    "print(area_full_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8b25d-611b-4771-a00c-9db7d876b9b0",
   "metadata": {},
   "source": [
    "## Defining the name of the output\n",
    "- This will dictate whether the distribution of points is weighted and or tuned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bb37397-9edb-4866-acf6-70eaf64f505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_classifier_name = 'Meuse_Max_untuned_balanced'\n",
    "# saved_classifier_name = 'Meuse_Max_untuned'\n",
    "saved_classifier_name = 'Meuse_HC_tuned'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e26e18d-333e-4e99-9f2b-fb7bd96d65ce",
   "metadata": {},
   "source": [
    "## The Random Forest has several hyperparameters that can be tuned\n",
    "\n",
    "- These are passed via a dictionary and are determined via tuning in another notebook\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3ab3226-6390-4105-adde-ed00fc1ff3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'number_of_trees':120,\n",
    "    'variables_per_split':8,\n",
    "    'minimum_leaf_population':1,\n",
    "    'bag_fraction':0.6,\n",
    "    'max_nodes':330,\n",
    "    'seed':0\n",
    "}\n",
    "\n",
    "\n",
    "classLoopParams = {'dataset':'CORINE',    #training dataset, no other than corine currently supported\n",
    "               'trainingClassLevel':1, #classLevel determines the level of corine class simplification\n",
    "               'customClassLevels':None,   #can provide some custom levels, not fully tested\n",
    "               'numClasses':5,            #if trainingClassLevel is 1 then there are 5 classes, level is 2 then there are 15, 3 is 44. (CORINE land cover class grouping)\n",
    "               'split':0.7,               #split the training and testing 0.7/0.3 (70% training, 30% accuracy testing). \n",
    "               'tileScale':2,            #tileScale higher number reduces likelihood of classifier running into a memory limit\n",
    "                'distribution':'weighted',  # can be weighted or balanced, weighting is done be area proportion\n",
    "               'weighting':list(area_full_pct),  # weighting based on area, percentages are passed , dictionaries do not pass to javascript code as well as lists\n",
    "               'year_classified': [1990, 2000, 2006, 2012, 2018],   # classification year is , for classifiers saved, the same as the years available in the training dataset\n",
    "                'hyperparameters': [hyperparameters['number_of_trees'],\n",
    "                                   hyperparameters['variables_per_split'],\n",
    "                                   hyperparameters['minimum_leaf_population'],\n",
    "                                   hyperparameters['bag_fraction'],\n",
    "                                    hyperparameters['max_nodes'],\n",
    "                                   hyperparameters['seed']] #hyperparameters are passed to  via a list, not a dictionary. \n",
    "              }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dddd40-74f7-4634-8072-5d2aed4a85db",
   "metadata": {},
   "source": [
    "## This loop is set to generate all classifiers, it will output CSV classifiers\n",
    "\n",
    "CSV classifiers save locally for all available training years in the corine dataset (or years set by the year classified loop parameter). The training area is the combined geometry of the whole meuse basin and are uploaded from client to server in the form of a decision Tree Ensemble. \n",
    "\n",
    "- Creating the classifier we also need to generate the training accuracy assessing  \"\" confusion matrix \"\"\n",
    "\n",
    "from the confusion matrix: (overall) accuracy, kappa coefficient, producer's accuracy, consumers'accuracy. \n",
    "\n",
    "This will be assessed three times in the report, from a \n",
    "\n",
    "1. balanced non-tuned () [1990, 2000, 2006, 2012, 2018]\n",
    "2. then weighted, non-tuned\n",
    "3. then a weighted tuned classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a90db04-5094-4472-a2f0-d913c471406f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin loop: 2023-04-12 12:23:33.210231\n",
      "2023-04-12 12:23:33.212229\n",
      "Dataset: Meuse, \n",
      "Catchment: Meuse_HC_tuned, \n",
      "Surface Reflectance Processing ...\n",
      "\n",
      "\n",
      "step 2: Initialize classification routine: 2023-04-12 12:23:34.363495\n",
      "classifier is tuned\n",
      "year classified 1990\n",
      "Adding Terrain...\n",
      "Generating GCP...\n",
      "Building classifier...\n",
      "saving...\n",
      "Asessing classifier accuracy...\n",
      "Asessing classifier validation accuracy...\n",
      "error_matrices [[ 82 100  10   0   0]\n",
      " [ 25 737  41   0   2]\n",
      " [  4  48 430   0   0]\n",
      " [  0   1   2   0   0]\n",
      " [  1   5   0   0   2]]\n",
      "Classifying...\n",
      "time for 1990 data = 0:08:41.762195\n",
      "step2: Done: 2023-04-12 12:32:16.125690, time taken: 0:08:41.762195\n",
      "\n",
      "Catchment: Meuse_HC_tuned, total time: 0:08:42.913461\n",
      "---------------\n",
      "END LOOP: Full routine finished: 2023-04-12 12:32:16.200006 \n",
      "Time taken: 0:08:42.989775\n"
     ]
    }
   ],
   "source": [
    "t0 = dt.today()\n",
    "\n",
    "confusion_matrices = []\n",
    "\n",
    "error_matrices = []\n",
    "\n",
    "accuracy_dictionaries = []\n",
    "\n",
    "classArea_df = pd.DataFrame()\n",
    "\n",
    "validation_combined_df = pd.DataFrame()\n",
    "\n",
    "print(f'begin loop: {t0}')\n",
    "\n",
    "aoi = Filtered_Sorted.filter(ee.Filter.inList('station_re', names))\n",
    "\n",
    "\n",
    "for i, ind in enumerate(sys_index):\n",
    "    \n",
    "    area = df1.loc[ind].area_km2\n",
    "    \n",
    "    t1 = dt.today()\n",
    "    \n",
    "    name = saved_classifier_name\n",
    "    \n",
    "    print(f'{t1}\\nDataset: {dataset}, \\nCatchment: {name}, \\nSurface Reflectance Processing ...\\n')\n",
    "    \n",
    "    annual_med = ltgee.buildSRcollection(startYear, endYear, startDay, endDay, aoi, maskThese, [''])#.map(clip_collection) #much slower when clipped\n",
    "    \n",
    "    annual_med_calc = ltgee.transformSRcollection(annual_med, bandList)\n",
    "\n",
    "    t3 = dt.today()\n",
    "    \n",
    "    print(f'\\nstep 2: Initialize classification routine: {t3}')\n",
    "    \n",
    "    if dataset == 'Meuse':\n",
    "        '''\n",
    "        \n",
    "        The years that we train on are not necessarily the same as the years we classify: \n",
    "        - first define the years to return that will be relevant to the decadal analysis (matching the hydroclimatic decades)\n",
    "        - Then use conditions to define which training set corresponds best to the image to be classified. \n",
    "        - LATER: Will need to use a trained classifier, perhaps the 'best performer', to classify the USA dataset. \n",
    "            - Best accuracy may be to take a classifier that samples all 4 categories\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        if saved_classifier_name[-7:]=='untuned' or saved_classifier_name[-8-8:]=='untuned_balanced':\n",
    "            tuned = 'not_tuned'\n",
    "            print('not_tuned classifier')\n",
    "        else:\n",
    "            tuned = 'tuned'\n",
    "            print('classifier is tuned')\n",
    "        \n",
    "        \n",
    "        \n",
    "        for j, yc in enumerate(classLoopParams['year_classified']):\n",
    "            '''\n",
    "            Define the training image, then the image to classify, adding slope and elevation bands\n",
    "\n",
    "            Corine Representative Classes:\n",
    "            || 1989 -> 1998 | 1999 -> 2001 | 2005 -> 2007 | 2011 -> 2012 | 2017 -> 2018 ||\n",
    "            ||   \"1990\"     |    \"2000\"    |    \"2006\"    |     \"2012\"   |     \"2018\"   ||\n",
    "\n",
    "            Training \"image year above\" --> classify the relevant Landsat date range below:\n",
    "            || 1984 -> 1998 | 1999 -> 2003 | 2004 -> 2009 | 2010 -> 2014 | 2015 -> ...  || \n",
    "            \n",
    "            '''\n",
    "            if yc >= 1984 and yc < 1999:\n",
    "                classImageYear = 1990\n",
    "\n",
    "            elif yc >= 1999 and yc < 2004:\n",
    "                classImageYear = 2000\n",
    "\n",
    "            elif yc >= 2004 and yc < 2010:\n",
    "                classImageYear = 2006\n",
    "\n",
    "            elif yc >= 2010 and yc < 2015:\n",
    "                classImageYear = 2012\n",
    "\n",
    "            elif yc >= 2015:\n",
    "                classImageYear = 2018\n",
    "\n",
    "            else:\n",
    "                print('ERROR: year to classify out of range[1984 +]')\n",
    "                break\n",
    "            \n",
    "            print('year classified', yc)\n",
    "            \n",
    "            #the image from the collection that we want to classify\n",
    "            imageFromCollection = ee.ImageCollection(annual_med_calc).filterDate(str(yc)+'-'+startDay, str(yc+1)+'-'+endDay).first()\n",
    "            \n",
    "            #image from training dataset e.g. CORINE is selected and simlified... \n",
    "            trainingClassImage = ltgee.createTrainingImage(str(classImageYear), \n",
    "                                                           classLoopParams['dataset'], \n",
    "                                                           classLoopParams['trainingClassLevel'], \n",
    "                                                           aoi)\n",
    "            \n",
    "            \n",
    "            #TODO: reproject to 30m native resolution\n",
    "            clipped = imageFromCollection.clip(aoi)\n",
    "            \n",
    "            #Adding the elevation and slope band calculations to each image\n",
    "            print('Adding Terrain...')\n",
    "            imageToClassify = ltgee.addTerrainBand(clipped, aoi)\n",
    "        \n",
    "            \n",
    "            #getting the date of the image and converting it from milliseconds since 1970 (Earth engines preferred datetime)\n",
    "            ms = msToDate(ee.Date(imageToClassify.get('system:time_start')).getInfo()['value'])\n",
    "            \n",
    "            print('Generating GCP...')\n",
    "            #the points used for training the classifier are randomly distibuted amongst the classes extracting a profile of spectral and terrain\n",
    "            points = ltgee.genGCP(trainingClassImage, \n",
    "                                  imageToClassify, \n",
    "                                  classLoopParams['numClasses'], #set at 5 usually\n",
    "                                  classLoopParams['split'],      #set at 70/30\n",
    "                                  classLoopParams['tileScale'], \n",
    "                                  aoi, \n",
    "                                  classLoopParams['distribution'], #either balanced or weighted\n",
    "                                  classLoopParams['weighting'])\n",
    "            \n",
    "            \n",
    "            # 70% of the points are allocated to training\n",
    "            training = points['training']\n",
    "            \n",
    "            # 30% of the points are allocated to classification\n",
    "            testing = points['testing']\n",
    "            \n",
    "            t5 = dt.today()\n",
    "            \n",
    "            print('Building classifier...')\n",
    "            \n",
    "            #classifier training with predefined number of trees using training points\n",
    "            classifier = ltgee.classifier(imageToClassify, training, tuned, classLoopParams['hyperparameters'])\n",
    "            \n",
    "            t6 = dt.today()\n",
    "            \n",
    "            #saving decision trees for later use\n",
    "            saveClassifierToCSV(classifier, name, yc)\n",
    "            \n",
    "            '''\n",
    "            Accuracy metrics saved, firstly the testing metrics, generated via the confusion matrix, then similarly the Validation accuracy metrics (from the 30% split test). \n",
    "            A dataframe is generated, populated with the overall accuracy, kappa score, fscore, producers accuracy and consumers accuracy. \n",
    "            \n",
    "            The matrices (confusion/training & error/validation) are saved as individual npy files, and upon completion of a complete loop the list of all matrices are saved as a combined list of matrices. \n",
    "            \n",
    "            '''\n",
    "            \n",
    "            print('Asessing classifier accuracy...')\n",
    "            \n",
    "            confusion_matrix = classifier.confusionMatrix()\n",
    "            \n",
    "            cm = np.array(confusion_matrix.getInfo())[1:, 1:]\n",
    "            \n",
    "            accuracy = confusion_matrix.accuracy().getInfo()\n",
    "            \n",
    "            kappa = confusion_matrix.kappa().getInfo()\n",
    "            \n",
    "            fscore = confusion_matrix.fscore(1).getInfo() #beta is one, the precision and recall are equally important\n",
    "            \n",
    "            producers_accuracy = confusion_matrix.producersAccuracy().getInfo()\n",
    "            \n",
    "            consumers_accuracy = confusion_matrix.consumersAccuracy().getInfo()\n",
    "            \n",
    "            np.save(f'../Outputs/{dataset}/Trees/{saved_classifier_name}_{tuned}_confusionMatrix_{classImageYear}.npy', arr=cm, allow_pickle=True)   #saving the confusionMatrix to allow for later access in assessing the bias\n",
    "            \n",
    "            confusion_matrices.append(cm)\n",
    "\n",
    "            acc_dict = {'train_valid':'training',\n",
    "                        'tree_name':f'{saved_classifier_name}_{tuned}',\n",
    "                        'year':classImageYear, \n",
    "                        'o_accuracy':accuracy,\n",
    "                        'kappa':kappa,\n",
    "                        'fscore1':fscore[1],\n",
    "                        'fscore2':fscore[2],\n",
    "                        'fscore3':fscore[3],\n",
    "                        'fscore4':fscore[4],\n",
    "                        'fscore5':fscore[5],\n",
    "                        'p_accuracy1':producers_accuracy[1][0],\n",
    "                        'p_accuracy2':producers_accuracy[2][0],\n",
    "                        'p_accuracy3':producers_accuracy[3][0],\n",
    "                        'p_accuracy4':producers_accuracy[4][0],\n",
    "                        'p_accuracy5':producers_accuracy[5][0],\n",
    "                        'c_accuracy1':consumers_accuracy[0][1],\n",
    "                       'c_accuracy2':consumers_accuracy[0][2],\n",
    "                       'c_accuracy3':consumers_accuracy[0][3],\n",
    "                       'c_accuracy4':consumers_accuracy[0][4],\n",
    "                       'c_accuracy5':consumers_accuracy[0][5]}\n",
    "            \n",
    "            acc_df = pd.DataFrame(acc_dict, index=[0])\n",
    "            acc_df.to_excel(f'../Outputs/{dataset}/Trees/{saved_classifier_name}_{tuned}_accuracy_metrics_{classImageYear}.xlsx')\n",
    "            \n",
    "            importance = ee.Dictionary(classifier.explain()).get('importance').getInfo()\n",
    "            \n",
    "            validation_combined_df = validation_combined_df.append(acc_df)\n",
    "            \n",
    "            pd.DataFrame(importance, index=[0]).to_excel(f'../Outputs/{dataset}/Trees/{saved_classifier_name}_{tuned}_importance_{classImageYear}.xlsx')\n",
    "            \n",
    "            '''\n",
    "            Validation metrics saved\n",
    "            '''\n",
    "            \n",
    "            print('Asessing classifier validation accuracy...')\n",
    "            \n",
    "            validated = testing.classify(classifier)\n",
    "            \n",
    "            test_matrix = validated.errorMatrix('landcover', 'classification')\n",
    "            \n",
    "            em = np.array(test_matrix.getInfo())[1:,1:]\n",
    "            \n",
    "            error_matrices.append(em)\n",
    "            \n",
    "            accuracy = test_matrix.accuracy().getInfo()\n",
    "            \n",
    "            kappa = test_matrix.kappa().getInfo()\n",
    "            \n",
    "            fscore = test_matrix.fscore(1).getInfo() #beta is one, the precision and recall are equally important\n",
    "            \n",
    "            producers_accuracy = test_matrix.producersAccuracy().getInfo()\n",
    "            \n",
    "            consumers_accuracy = test_matrix.consumersAccuracy().getInfo()\n",
    "            \n",
    "            error_matrices.append(em)\n",
    "            \n",
    "            print('error_matrices', em)\n",
    "            \n",
    "\n",
    "            val_dict = {'train_valid':'validation',\n",
    "                        'tree_name':f'{saved_classifier_name}_{tuned}',\n",
    "                        'year':classImageYear, \n",
    "                        'o_accuracy':accuracy,\n",
    "                        'kappa':kappa,\n",
    "                        'fscore1':fscore[1],\n",
    "                        'fscore2':fscore[2],\n",
    "                        'fscore3':fscore[3],\n",
    "                        'fscore4':fscore[4],\n",
    "                        'fscore5':fscore[5],\n",
    "                        'p_accuracy1':producers_accuracy[1][0],\n",
    "                        'p_accuracy2':producers_accuracy[2][0],\n",
    "                        'p_accuracy3':producers_accuracy[3][0],\n",
    "                        'p_accuracy4':producers_accuracy[4][0],\n",
    "                        'p_accuracy5':producers_accuracy[5][0],\n",
    "                        'c_accuracy1':consumers_accuracy[0][1],\n",
    "                       'c_accuracy2':consumers_accuracy[0][2],\n",
    "                       'c_accuracy3':consumers_accuracy[0][3],\n",
    "                       'c_accuracy4':consumers_accuracy[0][4],\n",
    "                       'c_accuracy5':consumers_accuracy[0][5]}\n",
    "                        \n",
    "            val_df = pd.DataFrame(val_dict, index=[0])\n",
    "            \n",
    "            val_df.to_excel(f'../Outputs/{dataset}/Trees/{saved_classifier_name}_{tuned}_validation_metrics_{classImageYear}.xlsx')\n",
    "            \n",
    "            np.save(f'../Outputs/{dataset}/Trees/{saved_classifier_name}_{tuned}_validationMatrix_{classImageYear}.npy', arr=em, allow_pickle=True)   #saving the confusionMatrix to allow for later access in assessing the bias\n",
    "            \n",
    "            print('Classifying...')\n",
    "            \n",
    "            classified = imageToClassify.classify(classifier)\n",
    "\n",
    "            df = dataframeAreas(i, j, classified, trainingClassImage, ms, classImageYear, f'{saved_classifier_name}_{tuned}', accuracy)\n",
    "                       \n",
    "            classArea_df = classArea_df.append(df)\n",
    "            \n",
    "            validation_combined_df = validation_combined_df.append(val_df)\n",
    "            \n",
    "            t2 = dt.today()\n",
    "            \n",
    "            print(f'time for {classImageYear} data = {t2 - t3}')\n",
    "            \n",
    "    else:\n",
    "        print('classification routine for this dataset is not yet provided for')\n",
    "    \n",
    "    t4 = dt.today()\n",
    "    print(f'step2: Done: {t4}, time taken: {t4-t3}')\n",
    "    print(f'\\nCatchment: {name}, total time: {t4-t1}\\n---------------')\n",
    "    \n",
    "    if ind == sys_index[0]:\n",
    "        break\n",
    "\n",
    "classArea_df.to_excel(f'../Outputs/{dataset}/RF_classification/{saved_classifier_name}_{tuned}_combinedAreas.xlsx')\n",
    "\n",
    "np.save(f'../Outputs/{dataset}/Trees/{saved_classifier_name}_{tuned}_confusionMatrices_All_{classImageYear}.npy', arr=confusion_matrices, allow_pickle=True)  \n",
    "\n",
    "np.save(f'../Outputs/{dataset}/Trees/{saved_classifier_name}_{tuned}_errorMatrices_All_{classImageYear}.npy', arr=error_matrices, allow_pickle=True)  \n",
    "\n",
    "\n",
    "tfinal = dt.today()\n",
    "\n",
    "print(f'END LOOP: Full routine finished: {tfinal} \\nTime taken: {tfinal-t0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae24546-802c-4fb4-9131-f9271cfe93da",
   "metadata": {},
   "source": [
    "1. Define t0 as the current date and time\n",
    "2. Create empty lists for the confusion matrices, error matrices and accuracy dictionaries\n",
    "3. Define an empty pandas dataframe classArea_df and validation_combined_df\n",
    "4. Define the AOI using a list of station_re names with an inList filter applied.\n",
    "5. For each element in sys_index do the following:\n",
    "     a. Define the area using the area_km2 value of the row in df1 indexed by ind\n",
    " \n",
    "     b. Use ltgee.buildSRcollection() to build an image collection of surface reflectance data for the aoi and the given year range startYear to endYear. The bandList is extracted.\n",
    " \n",
    "     c. Use ltgee.transformSRcollection() to calculate the median for each image in the collection.\n",
    " \n",
    "     d. Define name as the saved classifier name.\n",
    " \n",
    "     e. Based on the year that needs to be classified, select the training set for that year using ltgee.createTrainingImage().\n",
    " \n",
    "     f. Clip the image collection to the AOI using imageFromCollection.clip(aoi).\n",
    " \n",
    "     g. Add slope and elevation bands to each image in the clipped image collection using ltgee.addTerrainBand().\n",
    " \n",
    "     h. Generate a set of ground control points using ltgee.genGCP().\n",
    " \n",
    "     i. Allocate 70% of the ground control points to training and the remaining 30% to testing.\n",
    " \n",
    "     j. Train a random forest classifier using ltgee.classifier().\n",
    " \n",
    "     k. Save the decision tree and importance values as a CSV file.\n",
    " \n",
    "     l. Generate a confusion matrix for the training data and save it to an npy file.\n",
    " \n",
    "     m. Generate a dataframe containing accuracy metrics for the training data.\n",
    " \n",
    "     n. Generate an error matrix for the testing data and save it to an npy file.\n",
    " \n",
    "     o. Generate a dataframe containing accuracy metrics for the testing data.\n",
    " \n",
    "     p. Append the accuracy metrics for the testing data to validation_combined_df.\n",
    " \n",
    "     q. Use the classifier to classify the image collection.\n",
    " \n",
    "     r. Generate a pandas dataframe of areas classified for each class.\n",
    " \n",
    "6. Write classArea_df to an Excel file.\n",
    "\n",
    "    a. Save the confusion matrices and error matrices to npy files.\n",
    "\n",
    "    b. Write validation_combined_df to an Excel file.\n",
    "\n",
    "    c. Define tfinal as the current date and time.\n",
    "\n",
    "    d. Print the time taken to run the full loop.\n",
    "\n",
    "Code Explanation:\n",
    "\n",
    "The code is written in Python and is used to classify surface reflectance data. It is designed to be used with Google Earth Engine (GEE) to process satellite data. The script classifies each image in an image collection using a random forest algorithm. The output is a pandas dataframe containing areas classified for each class in each image. The code is structured such that each image is processed independently, allowing it to be run in parallel.\n",
    "\n",
    "The script starts by defining the current date and time using dt.today(). It then creates empty lists for the confusion matrices, error matrices, and accuracy dictionaries. The script then defines an empty pandas dataframe classArea_df and validation_combined_df.\n",
    "\n",
    "The AOI is defined using a list of station_re names and an inList filter. For each element in sys_index, the script defines the area, builds an image collection of surface reflectance data, calculates the median for each image in the collection, and defines the saved classifier name.\n",
    "\n",
    "Based on the year that needs to be classified, the script selects the training set and clips the image collection to the AOI. Slope and elevation bands are then added to each image in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93d66bec-d692-4ba6-8726-177a97c741bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07c6a092d7d4f3898cddf4bbe4f8884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[49.6453590639957, 5.058576000213446], controls=(ZoomControl(options=['position', 'zoom_in_text', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Map = geemap.Map()\n",
    "Map.setOptions('SATELLITE')\n",
    "Map.centerObject(aoi, 10)\n",
    "Map.addLayer(classified, {'bands':['classification'], 'min':1, 'max':5, 'palette':['#E6004D', '#FFFFA8', '#80FF00', '#A6A6FF', '#00CCF2']})\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056beaae-2adb-4bfe-b64f-ab597696525c",
   "metadata": {},
   "source": [
    "## This Loop Classifies the Set of Catchments from the hydroclimatic study\n",
    "## The years desired are set to maximum and the outputs are sent to a google asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3852b58-286e-4cd1-8813-d9ef74def4d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\n",
      " 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011\n",
      " 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022]\n"
     ]
    }
   ],
   "source": [
    "classifier_used = 'Meuse_HC_tuned'\n",
    "\n",
    "c_1990 = ml.csv_to_classifier(f'../Outputs/Meuse/Trees/{classifier_used}_1990')\n",
    "c_2000 = ml.csv_to_classifier(f'../Outputs/Meuse/Trees/{classifier_used}_2000')\n",
    "c_2006 = ml.csv_to_classifier(f'../Outputs/Meuse/Trees/{classifier_used}_2006')\n",
    "c_2012 = ml.csv_to_classifier(f'../Outputs/Meuse/Trees/{classifier_used}_2012')\n",
    "c_2018 = ml.csv_to_classifier(f'../Outputs/Meuse/Trees/{classifier_used}_2018')\n",
    "\n",
    "print(np.arange(1984, 2023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e47b1fa5-fdf0-4af7-816c-cfa89ee6f3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin loop: 2023-04-11 11:51:10.506959\n",
      "\n",
      "1/22 2023-04-11 11:51:10.507958\n",
      "Dataset: Meuse, \n",
      "Catchment: Meuse_Reupload_Test, \n",
      "Surface Reflectance Processing ...\n",
      "\n",
      "\n",
      "step 2: Initialize classification routine: 2023-04-11 11:51:11.654071\n",
      "2018\n",
      "classifying...\n"
     ]
    },
    {
     "ename": "EEException",
     "evalue": "Request payload size exceeds the limit: 10485760 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\BPD4a\\lib\\site-packages\\ee\\data.py:328\u001b[0m, in \u001b[0;36m_execute_cloud_call\u001b[1;34m(call, num_retries)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 328\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_retries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m googleapiclient\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mHttpError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BPD4a\\lib\\site-packages\\googleapiclient\\_helpers.py:134\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BPD4a\\lib\\site-packages\\googleapiclient\\http.py:915\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[1;34m(self, http, num_retries)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 915\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[1;31mHttpError\u001b[0m: <HttpError 400 when requesting https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/value:compute?prettyPrint=false&alt=json returned \"Request payload size exceeds the limit: 10485760 bytes.\". Details: \"Request payload size exceeds the limit: 10485760 bytes.\">",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mEEException\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 164\u001b[0m\n\u001b[0;32m    161\u001b[0m classified \u001b[38;5;241m=\u001b[39m imageToClassify\u001b[38;5;241m.\u001b[39mclassify(classifier)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m#assess the accuracy using the testing points, see where the confusion occurs\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlandcover\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassification\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, accuracy)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03mreturn tuple of important stuff\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m(year classified, year trained, accuracy, corine area:[1-5], masked area 6, classified area:[1-5], masked area 6,)\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BPD4a\\lib\\site-packages\\ee\\computedobject.py:98\u001b[0m, in \u001b[0;36mComputedObject.getInfo\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetInfo\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;124;03m\"\"\"Fetch and return information about this object.\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    The object can evaluate to anything.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomputeValue\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BPD4a\\lib\\site-packages\\ee\\data.py:738\u001b[0m, in \u001b[0;36mcomputeValue\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m workload_tag:\n\u001b[0;32m    736\u001b[0m   body[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkloadTag\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m workload_tag\n\u001b[1;32m--> 738\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_execute_cloud_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_get_cloud_api_resource\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_get_projects_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprettyPrint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BPD4a\\lib\\site-packages\\ee\\data.py:330\u001b[0m, in \u001b[0;36m_execute_cloud_call\u001b[1;34m(call, num_retries)\u001b[0m\n\u001b[0;32m    328\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39mexecute(num_retries\u001b[38;5;241m=\u001b[39mnum_retries)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m googleapiclient\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mHttpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 330\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m _translate_cloud_exception(e)\n",
      "\u001b[1;31mEEException\u001b[0m: Request payload size exceeds the limit: 10485760 bytes."
     ]
    }
   ],
   "source": [
    "'''\n",
    "This strategy performs well and is \n",
    "\n",
    "'''\n",
    "hyperparameters = {\n",
    "    'number_of_trees':120,\n",
    "    'variables_per_split':8,\n",
    "    'minimum_leaf_population':1,\n",
    "    'bag_fraction':0.6,\n",
    "    'max_nodes':330,\n",
    "    'seed':0\n",
    "}\n",
    "\n",
    "\n",
    "classLoopParams = {'dataset':'CORINE',    #training dataset, no other than corine currently supported\n",
    "               'trainingClassLevel':1, #classLevel determines the level of corine class simplification\n",
    "               'customClassLevels':None,   #can provide some custom levels, not fully tested\n",
    "               'numClasses':5,            #if trainingClassLevel is 1 then there are 5 classes, level is 2 then there are 15, 3 is 44. (CORINE land cover class grouping)\n",
    "               'split':0.7,               #split the training and testing 0.7/0.3 (70% training, 30% accuracy testing). \n",
    "               'tileScale':2,            #tileScale higher number reduces likelihood of classifier running into a memory limit\n",
    "                'distribution':'weighted',  # can be weighted or balanced, weighting is done be area proportion\n",
    "               'weighting':list(area_full_pct),  # weighting based on area, percentages are passed , dictionaries do not pass to javascript code as well as lists\n",
    "               'year_classified':np.arange,   # classification year is , for classifiers saved, the same as the years available in the training dataset\n",
    "                'hyperparameters': [hyperparameters['number_of_trees'],\n",
    "                                   hyperparameters['variables_per_split'],\n",
    "                                   hyperparameters['minimum_leaf_population'],\n",
    "                                   hyperparameters['bag_fraction'],\n",
    "                                    hyperparameters['max_nodes'],\n",
    "                                   hyperparameters['seed']] #hyperparameters are passed to  via a list, not a dictionary. \n",
    "              }\n",
    "\n",
    "\n",
    "t0 = dt.today()\n",
    "\n",
    "confusion_matrices = []\n",
    "\n",
    "error_matrices = []\n",
    "\n",
    "accuracy_dictionaries = []\n",
    "\n",
    "classArea_df = pd.DataFrame()\n",
    "\n",
    "validation_combined_df = pd.DataFrame()\n",
    "\n",
    "print(f'begin loop: {t0}')\n",
    "\n",
    "aoi = Filtered_Sorted.filter(ee.Filter.inList('station_re', names))\n",
    "\n",
    "for i, ind in enumerate(sys_index):\n",
    "    '''\n",
    "    The Loop cycles through all years in the timeseries available \n",
    "    '''\n",
    "    \n",
    "    name = 'Catchments Within the Hydroclimatic Study'\n",
    "    \n",
    "    area = df1.loc[ind].area_km2\n",
    "    \n",
    "    t1 = dt.today()\n",
    "    \n",
    "    print(f'\\n{i+1}/{len(sys_index)} {t1}\\nDataset: {dataset}, \\n {name}, \\nSurface Reflectance Processing ...\\n')\n",
    "    \n",
    "    \n",
    "    annual_med = ltgee.buildSRcollection(startYear, endYear, startDay, endDay, aoi, maskThese, ['slcOff'])#.map(clip_collection) #much slower when clipped\n",
    "    \n",
    "    annual_med_calc = ltgee.transformSRcollection(annual_med, bandList)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Which images are used in creating the annual composites?\n",
    "    -> return as a list of Landsat image IDs ** \n",
    "    ** can be used for exclusion if image quality is suboptimal upon later inspection.. important for small sample cases.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    t3 = dt.today()\n",
    "    \n",
    "    print(f'\\nstep 2: Initialize classification routine: {t3}')\n",
    "    \n",
    "    if dataset == 'Meuse':\n",
    "        '''\n",
    "        \n",
    "        The years that we train on are not necessarily the same as the years we classify: \n",
    "        - first define the years to return that will be relevant to the decadal analysis (matching the hydroclimatic decades)\n",
    "        - Then use conditions to define which training set corresponds best to the image to be classified. \n",
    "        - LATER: Will need to use a trained classifier, perhaps the 'best performer', to classify the USA dataset. \n",
    "            - Best accuracy may be to take a classifier that samples all 4 categories\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        year_classified = [2018] #np.arange(1984, 2023)\n",
    "        \n",
    "        for j, yc in enumerate(classLoopParams['year_classified']):\n",
    "            '''\n",
    "            Define the training image, then the image to classify, adding slope and elevation bands\n",
    "\n",
    "            Corine Representative Classes:\n",
    "            || 1989 -> 1998 | 1999 -> 2001 | 2005 -> 2007 | 2011 -> 2012 | 2017 -> 2018 ||\n",
    "            ||   \"1990\"     |    \"2000\"    |    \"2006\"    |     \"2012\"   |     \"2018\"   ||\n",
    "\n",
    "            Training \"image year above\" --> classify the relevant Landsat date range below:\n",
    "            || 1984 -> 1998 | 1999 -> 2003 | 2004 -> 2009 | 2010 -> 2014 | 2015 -> ...  || \n",
    "            '''\n",
    "            if yc >= 1984 and yc < 1999:\n",
    "                classifier = c_1990\n",
    "                classImageYear = 1990\n",
    "\n",
    "            elif yc >= 1999 and yc < 2004:\n",
    "                classifier = c_2000\n",
    "                classImageYear = 2000\n",
    "\n",
    "            elif yc >= 2004 and yc < 2010:\n",
    "                classifier = c_2006\n",
    "                classImageYear = 2006\n",
    "\n",
    "            elif yc >= 2010 and yc < 2015:\n",
    "                classifier = c_2012\n",
    "                classImageYear = 2012\n",
    "\n",
    "            elif yc >= 2015:\n",
    "                classifier = c_2018\n",
    "                classImageYear = 2018\n",
    "            else:\n",
    "                print('ERROR: year to classify out of range[1984 - 2022]')\n",
    "                break\n",
    "            \n",
    "            print(yc)\n",
    "\n",
    "            #the image from the collection that we want to classify\n",
    "            imageFromCollection = ee.ImageCollection(annual_med_calc).filterDate(str(yc)+'-'+startDay, str(yc+1)+'-'+endDay).first()\n",
    "            \n",
    "            #image from training dataset e.g. CORINE is selected and simlified... \n",
    "            trainingClassImage = ltgee.createTrainingImage(str(classImageYear), \n",
    "                                                           classLoopParams['dataset'], \n",
    "                                                           classLoopParams['trainingClassLevel'], \n",
    "                                                           aoi)\n",
    "            \n",
    "            \n",
    "            #TODO: reproject to 30m native resolution\n",
    "            clipped = imageFromCollection.clip(aoi)\n",
    "            \n",
    "            #Adding the elevation and slope band calculations to each image\n",
    "            imageToClassify = ltgee.addTerrainBand(clipped, aoi)\n",
    "        \n",
    "            \n",
    "            #getting the date of the image and converting it from milliseconds since 1970 (Earth engines preferred datetime)\n",
    "            ms = msToDate(ee.Date(imageToClassify.get('system:time_start')).getInfo()['value'])\n",
    "            \n",
    "            #the points used for training the classifier are randomly distibuted amongst the classes extracting a profile of spectral and terrain\n",
    "            points = ltgee.genGCP(trainingClassImage,\n",
    "                                  imageToClassify, \n",
    "                                  classLoopParams['numClasses'], \n",
    "                                  classLoopParams['split'], \n",
    "                                  classLoopParams['tileScale'], \n",
    "                                  aoi, \n",
    "                                  classLoopParams['distribution'], \n",
    "                                  classLoopParams['weighting'])\n",
    "            \n",
    "            \n",
    "            # 70% of the points are allocated to training\n",
    "            training = points['training']\n",
    "            \n",
    "            #30% of the points are allocated to classification\n",
    "            testing = points['testing']\n",
    "            \n",
    "            t5 = dt.today()\n",
    "            \n",
    "            print('classifying...')\n",
    "            \n",
    "            #classifier training with predefined number of trees using training points\n",
    "            classifier = c_2018 #ltgee.classifier(imageToClassify, training, tuned, classLoopParams['hyperparameters'])\n",
    "            \n",
    "            t6 = dt.today()\n",
    "            \n",
    "            #classifying image using the training\n",
    "            classified = imageToClassify.classify(classifier)\n",
    "            \n",
    "            #assess the accuracy using the testing points, see where the confusion occurs\n",
    "            accuracy = testing.classify(classifier).errorMatrix('landcover', 'classification').accuracy().getInfo()\n",
    "            \n",
    "            print('accuracy:', accuracy)\n",
    "\n",
    "\n",
    "            df = dataframeAreas(i, j, classified, trainingClassImage, ms, classImageYear, name, accuracy)\n",
    "            \n",
    "            \n",
    "            \n",
    "            classArea_df = classArea_df.append(df)\n",
    "            \n",
    "    else:\n",
    "        print('classification routine for this dataset is not yet provided for')\n",
    "    \n",
    "    t4 = dt.today()\n",
    "    print(f'step2: Done: {t4}, time taken: {t4-t3}')\n",
    "    print(f'\\nCatchment: {name}, total time: {t4-t1}\\n---------------')\n",
    "    \n",
    "    if ind == sys_index[0]:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "tfinal = dt.today()\n",
    "\n",
    "print(f'END LOOP: Full routine finished: {tfinal} \\nTime taken: {tfinal-t0}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
